{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the Preprocessed Data",
   "id": "458083ce2249405a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Indicate if the script is running on Google Colab or not\n",
    "using_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if using_colab:\n",
    "    # Connect Google Drive to Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    \n",
    "    # Install necessary packages\n",
    "    !pip install numpy pandas scikit-learn matplotlib seaborn imbalanced-learn shap\n",
    "    \n",
    "    # Path for Google Colab\n",
    "    project_root = '/content/gdrive/MyDrive/oc_projet_4/' \n",
    "else:\n",
    "    # Get the current working directory as base directory for the notebook\n",
    "    base_dir = os.getcwd()\n",
    "    \n",
    "    # Adjust the project root path relatively to where the notebook is located\n",
    "    # Assuming the notebook is inside a 'notebooks' directory and we need to go up one level to access project root\n",
    "    project_root = os.path.join(base_dir, '..')\n",
    "\n",
    "# Clean output of cell\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n"
   ],
   "id": "afc42854a3e4780d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import gc\n",
    "\n",
    "# Load the CSV file\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "\n",
    "app_train_domain = pd.read_csv(train_domain_path)  # Load the training data\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = app_train_domain.drop(columns=['TARGET']).values  # Features for training\n",
    "y = app_train_domain['TARGET'].values  # Target variable for training\n",
    "\n",
    "print('Data loaded successfully.')\n",
    "print(f'X shape: {X.shape}')  # X is used for training the model\n",
    "print(f'y shape: {y.shape}')  # y is the target variable for training"
   ],
   "id": "bd48a92e09961fda",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "X_train shape: (307511, 247)\n",
      "X_val shape: (61503, 247)\n",
      "X_test shape: (48744, 248)\n",
      "y_train shape: (307511,)\n",
      "y_val shape: (61503,)\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T13:45:27.507154Z",
     "start_time": "2024-06-17T13:45:27.479128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the first few values of y_train\n",
    "print('First 10 values of y_train:', y_train[:10])\n",
    "\n",
    "# Check the unique values in y_train\n",
    "print('Unique values in y_train:', np.unique(y_train))\n",
    "\n",
    "# Check for missing values in y_train\n",
    "print('Number of missing values in y_train:', pd.Series(y_train).isnull().sum())"
   ],
   "id": "e3ec5c9abc0dbc05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values of y_train: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Unique values in y_train: [0. 1.]\n",
      "Number of missing values in y_train: 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T13:45:36.051492Z",
     "start_time": "2024-06-17T13:45:27.807942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, zscore\n",
    "\n",
    "using_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if using_colab:\n",
    "    # Load the CSV file using the relative path\n",
    "    train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "else:\n",
    "    # Get the current working directory as base directory for the notebook\n",
    "    base_dir = os.getcwd()\n",
    "    \n",
    "    # Adjust the project root path relatively to where the notebook is located\n",
    "    # Assuming the notebook is inside a 'notebooks' directory and we need to go up one level to access project root\n",
    "    project_root = os.path.join(base_dir, '..')\n",
    "\n",
    "# Load the CSV file using relative path\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "\n",
    "train_data = pd.read_csv(train_domain_path)  # Load the training data\n",
    "\n",
    "# Features to analyze\n",
    "features_to_analyze = [\n",
    "    'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM',\n",
    "    'DAYS_EMPLOYED_PERCENT', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'\n",
    "]\n",
    "\n",
    "# Shapiro-Wilk Test for normality\n",
    "def test_normality(df, features):\n",
    "    normality_results = {}\n",
    "    for feature in features:\n",
    "        stat, p = shapiro(df[feature].dropna())  # Ignore missing values for the test\n",
    "        normality_results[feature] = {'Statistic': stat, 'p-value': p, 'Normal': p > 0.05}\n",
    "    return normality_results\n",
    "\n",
    "# Detecting outliers using z-score\n",
    "def detect_outliers(df, features):\n",
    "    outliers_results = {}\n",
    "    for feature in features:\n",
    "        z_scores = zscore(df[feature].dropna())  # Ignore missing values for z-score calculation\n",
    "        outliers = np.where(np.abs(z_scores) > 3)[0]  # Outliers with z-score > 3\n",
    "        outliers_results[feature] = df[feature].iloc[outliers]\n",
    "    return outliers_results\n",
    "\n",
    "# Execute the functions\n",
    "normality_results = test_normality(train_data, features_to_analyze)\n",
    "outliers_results = detect_outliers(train_data, features_to_analyze)\n",
    "\n",
    "# Display the results\n",
    "print(\"Normality Test Results (Shapiro-Wilk):\")\n",
    "for feature, result in normality_results.items():\n",
    "    print(f\"{feature}: Statistic={result['Statistic']}, p-value={result['p-value']}, Normal={result['Normal']}\")\n",
    "\n",
    "print(\"\\nDetected Outliers:\")\n",
    "for feature, outliers in outliers_results.items():\n",
    "    print(f\"{feature}: {len(outliers)} outliers\")\n",
    "    print(outliers)\n"
   ],
   "id": "41eb48546b60ab7a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 307511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normality Test Results (Shapiro-Wilk):\n",
      "CREDIT_INCOME_PERCENT: Statistic=0.8683387382469063, p-value=7.017665490743911e-144, Normal=False\n",
      "ANNUITY_INCOME_PERCENT: Statistic=0.9120356975923316, p-value=1.7391184523648002e-131, Normal=False\n",
      "CREDIT_TERM: Statistic=0.8813492261004188, p-value=1.2529197766384718e-140, Normal=False\n",
      "DAYS_EMPLOYED_PERCENT: Statistic=0.8455920689876675, p-value=6.2034854179349085e-149, Normal=False\n",
      "EXT_SOURCE_1: Statistic=0.8413734001601811, p-value=8.494600632668699e-150, Normal=False\n",
      "EXT_SOURCE_2: Statistic=0.9267468129817444, p-value=4.763031590479468e-126, Normal=False\n",
      "EXT_SOURCE_3: Statistic=0.9644152629779538, p-value=9.85564431307928e-106, Normal=False\n",
      "DAYS_BIRTH: Statistic=0.9696564518865638, p-value=1.709357664268864e-101, Normal=False\n",
      "\n",
      "Detected Outliers:\n",
      "CREDIT_INCOME_PERCENT: 4340 outliers\n",
      "176       12.835714\n",
      "270       13.754462\n",
      "371       13.611000\n",
      "380       12.112000\n",
      "678       34.916667\n",
      "            ...    \n",
      "307369    13.742400\n",
      "307401    12.729433\n",
      "307443    16.217176\n",
      "307476    13.052667\n",
      "307480    16.666667\n",
      "Name: CREDIT_INCOME_PERCENT, Length: 4340, dtype: float64\n",
      "ANNUITY_INCOME_PERCENT: 3792 outliers\n",
      "60        0.483275\n",
      "678       1.373917\n",
      "719       0.494467\n",
      "733       0.504385\n",
      "779       0.761000\n",
      "            ...   \n",
      "307247    0.486640\n",
      "307363    0.735692\n",
      "307405    0.533727\n",
      "307443    0.582471\n",
      "307480    0.487333\n",
      "Name: ANNUITY_INCOME_PERCENT, Length: 3792, dtype: float64\n",
      "CREDIT_TERM: 106 outliers\n",
      "7474      0.121201\n",
      "7619      0.121292\n",
      "7753      0.124429\n",
      "13055     0.124428\n",
      "15537     0.121201\n",
      "            ...   \n",
      "289178    0.124428\n",
      "297906    0.121500\n",
      "297961    0.121388\n",
      "300090    0.124414\n",
      "300382    0.121388\n",
      "Name: CREDIT_TERM, Length: 106, dtype: float64\n",
      "DAYS_EMPLOYED_PERCENT: 6975 outliers\n",
      "49       -0.521408\n",
      "95       -0.527185\n",
      "127      -0.529192\n",
      "249      -0.517723\n",
      "259      -0.625676\n",
      "            ...   \n",
      "307397   -0.531505\n",
      "307408   -0.529349\n",
      "307413   -0.591815\n",
      "307502   -0.543859\n",
      "307508   -0.529266\n",
      "Name: DAYS_EMPLOYED_PERCENT, Length: 6975, dtype: float64\n",
      "EXT_SOURCE_1: 1593 outliers\n",
      "0         0.083037\n",
      "128       0.053183\n",
      "297       0.068260\n",
      "415       0.084902\n",
      "470       0.040684\n",
      "            ...   \n",
      "306896    0.065634\n",
      "307027    0.081045\n",
      "307126    0.072247\n",
      "307338    0.932105\n",
      "307448    0.073452\n",
      "Name: EXT_SOURCE_1, Length: 1593, dtype: float64\n",
      "EXT_SOURCE_2: 0 outliers\n",
      "Series([], Name: EXT_SOURCE_2, dtype: float64)\n",
      "EXT_SOURCE_3: 0 outliers\n",
      "Series([], Name: EXT_SOURCE_3, dtype: float64)\n",
      "DAYS_BIRTH: 0 outliers\n",
      "Series([], Name: DAYS_BIRTH, dtype: int64)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Analysis and Interpretation\n",
    "\n",
    "#### Normality Test Results (Shapiro-Wilk)\n",
    "\n",
    "The Shapiro-Wilk test was applied to each feature to assess normality. Here are the results:\n",
    "\n",
    "- **CREDIT_INCOME_PERCENT**: Statistic=0.868, p-value=7.018e-144, Normal=False\n",
    "- **ANNUITY_INCOME_PERCENT**: Statistic=0.912, p-value=1.739e-131, Normal=False\n",
    "- **CREDIT_TERM**: Statistic=0.881, p-value=1.253e-140, Normal=False\n",
    "- **DAYS_EMPLOYED_PERCENT**: Statistic=0.846, p-value=6.203e-149, Normal=False\n",
    "- **EXT_SOURCE_1**: Statistic=0.841, p-value=8.495e-150, Normal=False\n",
    "- **EXT_SOURCE_2**: Statistic=0.927, p-value=4.763e-126, Normal=False\n",
    "- **EXT_SOURCE_3**: Statistic=0.964, p-value=9.856e-106, Normal=False\n",
    "- **DAYS_BIRTH**: Statistic=0.970, p-value=1.709e-101, Normal=False\n",
    "\n",
    "**Interpretation**:\n",
    "- All the p-values are significantly smaller than 0.05, which means that none of the features follow a normal distribution. This suggests that standardization (which assumes normality) may not be the best preprocessing step. Instead, other scaling methods like Min-Max Scaling or Robust Scaler may be more appropriate.\n",
    "\n",
    "#### Detected Outliers\n",
    "\n",
    "The z-score method was used to detect outliers in each feature. Here are the results:\n",
    "\n",
    "- **CREDIT_INCOME_PERCENT**: 4340 outliers\n",
    "- **ANNUITY_INCOME_PERCENT**: 3792 outliers\n",
    "- **CREDIT_TERM**: 106 outliers\n",
    "- **DAYS_EMPLOYED_PERCENT**: 6975 outliers\n",
    "- **EXT_SOURCE_1**: 1593 outliers\n",
    "- **EXT_SOURCE_2**: 0 outliers\n",
    "- **EXT_SOURCE_3**: 0 outliers\n",
    "- **DAYS_BIRTH**: 0 outliers\n",
    "\n",
    "**Interpretation**:\n",
    "- **CREDIT_INCOME_PERCENT**, **ANNUITY_INCOME_PERCENT**, **DAYS_EMPLOYED_PERCENT**, and **EXT_SOURCE_1** have a significant number of outliers. This indicates that these features have extreme values which could potentially skew the results of the analysis or modeling.\n",
    "- **EXT_SOURCE_2**, **EXT_SOURCE_3**, and **DAYS_BIRTH** do not have any outliers detected with the z-score method, suggesting these features are more stable and less prone to extreme values.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Given these findings, here are some recommendations for preprocessing and modeling:\n",
    "\n",
    "1. **Normalization/Scaling**:\n",
    "   - Since the features do not follow a normal distribution, consider using **Min-Max Scaling** or **Robust Scaler** instead of standardization. Robust Scaler is particularly useful for features with many outliers as it scales the data using statistics that are robust to outliers.\n",
    "\n",
    "2. **Handling Outliers**:\n",
    "   - For features with significant outliers, consider applying techniques such as:\n",
    "     - **Winsorizing**: Capping the extreme values.\n",
    "     - **Transformation**: Applying log, square root, or other transformations to reduce the effect of outliers.\n",
    "     - **Removal**: Removing data points that are identified as outliers, although this should be done cautiously to avoid losing valuable information.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Investigate if some of these outliers are due to data entry errors or other anomalies that can be corrected.\n",
    "   - For features with many outliers, you can create additional features that capture the presence of outliers, e.g., a binary feature indicating if a value is an outlier or not.\n",
    "\n",
    "4. **Modeling**:\n",
    "   - Use models that are less sensitive to outliers, such as tree-based methods (e.g., Random Forest, Gradient Boosting) which are generally more robust to outliers.\n",
    "   - Ensure that you have robust cross-validation practices to evaluate the impact of these preprocessing steps.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Implement Min-Max Scaling or Robust Scaling on the features.\n",
    "- Apply outlier handling techniques as needed.\n",
    "- Re-evaluate the performance of your model with these preprocessing steps.\n",
    "- Ensure that your evaluation metrics align with the business goals, such as minimizing the cost of prediction errors as outlined in your project requirements.\n",
    "\n",
    "By following these recommendations, you can ensure that your data preprocessing is robust and your models are well-prepared to handle the characteristics of your dataset."
   ],
   "id": "3168660548360e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.Hyperparameter Tuning via GridSearchCV and Imbalanced-learn Pipeline",
   "id": "cbe789d731d07bcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Set Up Resampling Techniques**: Define over-sampling (SMOTE) and under-sampling (RandomUnderSampler) methods to handle class imbalance in the dataset.\n",
    "2. **Initialize Models**: Create instances of RandomForestClassifier and RidgeClassifier for classification tasks.\n",
    "3. **Define Hyperparameter Grids**: Specify parameter grids for tuning hyperparameters of the classifiers using GridSearchCV.\n",
    "4. **Create Pipelines**: Construct pipelines to integrate resampling techniques with the classifiers.\n",
    "5. **Perform Hyperparameter Tuning**: Use GridSearchCV to find the best hyperparameters and resampling techniques for each classifier, evaluating them using cross-validation and storing the best models."
   ],
   "id": "6ea95384359ab660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, confusion_matrix, classification_report\n",
    "import gc\n",
    "\n",
    "# Custom cost metric function\n",
    "def cost_metric(y_true, y_pred):\n",
    "    false_negatives = ((y_true == 1) & (y_pred == 0)).sum()  # Count the false negatives\n",
    "    false_positives = ((y_true == 0) & (y_pred == 1)).sum()  # Count the false positives\n",
    "    return 10 * false_negatives + false_positives  # Return the cost metric\n",
    "\n",
    "# Create a scorer using the custom cost metric\n",
    "cost_scorer = make_scorer(cost_metric, greater_is_better=False)  # Setting greater_is_better to False because we want to minimize the cost\n",
    "\n",
    "# Function to compute ROC AUC for models with or without predict_proba\n",
    "def compute_roc_auc(model, X, y):\n",
    "    if hasattr(model, \"predict_proba\"):  # Check if the model has predict_proba attribute\n",
    "        y_proba = model.predict_proba(X)[:, 1]  # Get the probability of the positive class\n",
    "    elif hasattr(model, \"decision_function\"):  # Check if the model has decision_function attribute\n",
    "        y_proba = model.decision_function(X)  # Get the decision function scores\n",
    "    else:\n",
    "        y_proba = model.predict(X)  # Use the model's predictions as the scores\n",
    "    return roc_auc_score(y, y_proba)  # Compute ROC AUC score\n",
    "\n",
    "# Load the data\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "app_train_domain = pd.read_csv(train_domain_path)  # Load the training data\n",
    "\n",
    "# Extract the features and target\n",
    "X = app_train_domain.drop(columns=['TARGET']).values  # Features\n",
    "y = app_train_domain['TARGET'].values  # Target variable\n",
    "\n",
    "# Define the resampling techniques\n",
    "over_sampler = SMOTE(random_state=42)  # Over-sampling method\n",
    "under_sampler = RandomUnderSampler(random_state=42)  # Under-sampling method\n",
    "\n",
    "# Define the models\n",
    "rf = RandomForestClassifier(random_state=42)  # Random Forest Classifier\n",
    "ridge = RidgeClassifier()  # Create an instance of the Ridge Classifier\n",
    "\n",
    "# Define the parameter grids for GridSearchCV with a larger hyperparameter space\n",
    "param_grid_rf = {  # Parameter grid for Random Forest\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'classifier__max_depth': [10, 20, 30],  # Maximum depth of the tree\n",
    "    'classifier__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "param_grid_ridge = {  # Parameter grid for Ridge Classifier\n",
    "    'classifier__alpha': [1.0, 0.1, 0.01, 0.001]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Define pipelines with Robust Scaling\n",
    "pipeline_rf_over = Pipeline([  # Pipeline for Random Forest with over-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('oversample', over_sampler),  # Over-sampling method\n",
    "    ('classifier', rf)  # Random Forest Classifier\n",
    "])\n",
    "\n",
    "pipeline_rf_under = Pipeline([  # Pipeline for Random Forest with under-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('undersample', under_sampler),  # Under-sampling method\n",
    "    ('classifier', rf)  # Random Forest Classifier\n",
    "])\n",
    "\n",
    "pipeline_rf_weight = Pipeline([  # Pipeline for Random Forest with class weight adjustment\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))  # Random Forest Classifier with class weight adjustment\n",
    "])\n",
    "\n",
    "pipeline_ridge_over = Pipeline([  # Pipeline for Ridge Classifier with over-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('oversample', over_sampler),  # Over-sampling method\n",
    "    ('classifier', ridge)  # Ridge Classifier\n",
    "])\n",
    "\n",
    "pipeline_ridge_under = Pipeline([  # Pipeline for Ridge Classifier with under-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('undersample', under_sampler),  # Under-sampling method\n",
    "    ('classifier', ridge)  # Ridge Classifier\n",
    "])\n",
    "\n",
    "pipeline_ridge_weight = Pipeline([  # Pipeline for Ridge Classifier with class weight adjustment\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('classifier', RidgeClassifier(class_weight='balanced'))  # Ridge Classifier with class weight adjustment\n",
    "])\n",
    "\n",
    "# Perform GridSearchCV for each pipeline and store the best estimators\n",
    "grids = [\n",
    "    (pipeline_rf_over, param_grid_rf, 'Random Forest with Over-sampling'),  # pipeline, parameter grid, name\n",
    "    (pipeline_rf_under, param_grid_rf, 'Random Forest with Under-sampling'),\n",
    "    (pipeline_rf_weight, param_grid_rf, 'Random Forest with Class Weight'),\n",
    "    (pipeline_ridge_over, param_grid_ridge, 'Ridge with Over-sampling'),\n",
    "    (pipeline_ridge_under, param_grid_ridge, 'Ridge with Under-sampling'),\n",
    "    (pipeline_ridge_weight, param_grid_ridge, 'Ridge with Class Weight')\n",
    "]\n",
    "\n",
    "best_estimators = {}  # Dictionary to store the best estimators\n",
    "roc_auc_scores = {}  # Dictionary to store the ROC AUC scores for each model\n",
    "\n",
    "for pipeline, param_grid, name in grids:  # Iterate over the pipelines\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=cost_scorer, cv=3, n_jobs=-1)  # GridSearchCV for hyperparameter tuning\n",
    "    grid_search.fit(X, y)  # Fit the model for training data\n",
    "    best_estimators[name] = grid_search.best_estimator_  # Store the best estimator\n",
    "    # Use cross_val_predict to get predictions for ROC AUC calculation\n",
    "    y_pred = cross_val_predict(grid_search.best_estimator_, X, y, cv=3, method='predict_proba')[:, 1]\n",
    "    roc_auc = roc_auc_score(y, y_pred)  # Compute ROC AUC\n",
    "    roc_auc_scores[name] = roc_auc  # Store the ROC AUC score\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")  # Print the best parameters\n",
    "    print(f\"Best ROC AUC for {name}: {roc_auc}\")  # Print the best ROC AUC score\n",
    "\n",
    "# Select the best model based on ROC AUC scores\n",
    "best_model_name = max(roc_auc_scores, key=roc_auc_scores.get)  # Key is a function that defines the sorting criteria\n",
    "best_model_final = best_estimators[best_model_name]  # Get the best model\n",
    "print(f\"\\nBest model: {best_model_name} with ROC AUC: {roc_auc_scores[best_model_name]}\")\n",
    "\n",
    "# Split the data into training and validation sets for independent evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Predict the target variable on the validation set\n",
    "y_val_pred = best_model_final.predict(X_val)  # Predict the target variable on the validation set\n",
    "y_val_proba = best_model_final.predict_proba(X_val)[:, 1] if hasattr(best_model_final, \"predict_proba\") else best_model_final.decision_function(X_val) if hasattr(best_model_final, \"decision_function\") else best_model_final.predict(X_val)  # Get the predicted probabilities\n",
    "\n",
    "# Compute confusion matrix for validation set\n",
    "conf_matrix_val = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Confusion Matrix (Validation):\")\n",
    "print(conf_matrix_val)\n",
    "\n",
    "# Compute ROC AUC score for validation set using probabilities\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"ROC AUC (Validation): {roc_auc_val}\")\n",
    "\n",
    "# Ensure ROC AUC < 0.82 for validation set\n",
    "if roc_auc_val >= 0.82:  # Check if ROC AUC score is greater than or equal to 0.82 otherwise issue a warning; 0.82 is a threshold to avoid overfitting\n",
    "    print(\"Warning: ROC AUC score (Validation) is greater than or equal to 0.82. Model might be overfitting.\")\n",
    "\n",
    "# Print classification report for validation set\n",
    "print(\"Classification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print the custom cost metric\n",
    "validation_cost = cost_metric(y_val, y_val_pred)\n",
    "print(f\"Validation Cost: {validation_cost}\")\n",
    "\n",
    "gc.collect()  # Garbage collection to free up memory. Works only in Jupyter notebooks.\n",
    "\n"
   ],
   "id": "4a2e0f29127ef7b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. **Random Forest with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__max_depth': 20, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.7025714300577274`\n",
    "\n",
    "2. **Random Forest with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__max_depth': 20, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.7393607794366517`\n",
    "\n",
    "3. **Random Forest with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__max_depth': 10, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.7284622555457663`\n",
    "\n",
    "4. **Ridge with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0}`\n",
    "   - **Best ROC AUC**: `0.7413334928962128`\n",
    "\n",
    "5. **Ridge with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0}`\n",
    "   - **Best ROC AUC**: `0.7449914888258974`\n",
    "\n",
    "6. **Ridge with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0}`\n",
    "   - **Best ROC AUC**: `0.7468065211237337`\n",
    "\n",
    "#### Observations\n",
    "\n",
    "1. **Random Forest**:\n",
    "   - The best parameters for both over-sampling and under-sampling are the same, suggesting that the model configuration is robust across different resampling strategies.\n",
    "   - The ROC AUC for under-sampling (0.7394) is higher than for over-sampling (0.7026), indicating that under-sampling might be a better strategy for the Random Forest model in this case.\n",
    "   - Using class weight adjustment in Random Forest shows an improvement in ROC AUC compared to over-sampling but still underperforms compared to under-sampling.\n",
    "\n",
    "2. **Ridge Classifier**:\n",
    "   - The best parameters for both over-sampling and under-sampling are the same.\n",
    "   - The ROC AUC values are very close for over-sampling (0.7413) and under-sampling (0.7450), indicating that both resampling strategies perform similarly well for the Ridge Classifier.\n",
    "   - Using class weight adjustment in Ridge Classifier gives a slightly better ROC AUC (0.7468), making it the best-performing method overall.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Random Forest** performs best with under-sampling, suggesting that reducing the number of majority class samples can help the model learn better.\n",
    "- **Ridge Classifier** performs best with class weight adjustment, indicating that this method handles class imbalance effectively.\n",
    "- Overall, class weight adjustment in the Ridge Classifier achieved the highest ROC AUC, making it the most effective method for this dataset."
   ],
   "id": "75f86ebcee7f818b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.Evaluate the Best Model",
   "id": "97552594f71e4aeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Select the Best Model**: Identify and select the best model from the hyperparameter-tuned models stored in `best_estimators` based on their performance on the test set.\n",
    "2. **Make Predictions**: Use the selected best model to predict the target variable on the test dataset.\n",
    "3. **Evaluate Performance**: Calculate the confusion matrix and ROC AUC score to assess the model's performance on the test data.\n",
    "4. **Check for Overfitting**: Ensure that the ROC AUC score is below 0.82 to avoid overfitting, issuing a warning if the score is higher.\n",
    "5. **Generate Detailed Metrics**: Print a comprehensive classification report including precision, recall, and F1-score for a detailed evaluation of the model's performance."
   ],
   "id": "3208e4b1a5c980ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results\n",
    "\n",
    "#### Confusion Matrix\n",
    "```\n",
    "Confusion Matrix (Validation):\n",
    "[[39011 17543]\n",
    " [ 1596  3353]]\n",
    "```\n",
    "The confusion matrix shows:\n",
    "- **True Negatives (TN)**: 39,011 (clients correctly identified as not defaulting)\n",
    "- **False Positives (FP)**: 17,543 (clients incorrectly identified as defaulting)\n",
    "- **False Negatives (FN)**: 1,596 (clients incorrectly identified as not defaulting)\n",
    "- **True Positives (TP)**: 3,353 (clients correctly identified as defaulting)\n",
    "\n",
    "#### ROC AUC Score\n",
    "```\n",
    "ROC AUC (Validation): 0.7493240652562565\n",
    "```\n",
    "The ROC AUC score is approximately 0.75, indicating a reasonable ability of the model to distinguish between defaulting and non-defaulting clients.\n",
    "\n",
    "#### Classification Report\n",
    "```\n",
    "Classification Report (Validation):\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.96      0.69      0.80     56554\n",
    "         1.0       0.16      0.68      0.26      4949\n",
    "\n",
    "    accuracy                           0.69     61503\n",
    "   macro avg       0.56      0.68      0.53     61503\n",
    "weighted avg       0.90      0.69      0.76     61503\n",
    "```\n",
    "- **Class 0 (Non-defaulting clients)**:\n",
    "  - **Precision**: 0.96 (The proportion of clients predicted not to default that actually did not default)\n",
    "  - **Recall**: 0.69 (The proportion of actual non-defaulting clients correctly predicted)\n",
    "  - **F1-Score**: 0.80 (Harmonic mean of precision and recall)\n",
    "  - **Support**: 56,554 (Number of actual non-defaulting clients)\n",
    "\n",
    "- **Class 1 (Defaulting clients)**:\n",
    "  - **Precision**: 0.16 (The proportion of clients predicted to default that actually defaulted)\n",
    "  - **Recall**: 0.68 (The proportion of actual defaulting clients correctly predicted)\n",
    "  - **F1-Score**: 0.26 (Harmonic mean of precision and recall, which is low due to precision being low)\n",
    "  - **Support**: 4,949 (Number of actual defaulting clients)\n",
    "\n",
    "- **Overall Metrics**:\n",
    "  - **Accuracy**: 0.69 (The proportion of total correct predictions)\n",
    "  - **Macro Average**: Average precision, recall, and F1-score for both classes (treating all classes equally)\n",
    "  - **Weighted Average**: Average precision, recall, and F1-score for both classes (considering the support of each class)\n",
    "\n",
    "### Observations and Insights\n",
    "1. **Model Performance**:\n",
    "   - The model shows a significant improvement in recall for defaulting clients (0.68), meaning it identified a large proportion of defaulting clients correctly.\n",
    "   - However, the precision for defaulting clients is quite low (0.16), indicating many false positives.\n",
    "   - The model performs well in predicting non-defaulting clients (Class 0) with high precision (0.96) and reasonable recall (0.69).\n",
    "\n",
    "2. **ROC AUC Score**:\n",
    "   - The ROC AUC score of 0.75 indicates that the model has a good ability to discriminate between defaulting and non-defaulting clients.\n",
    "\n",
    "3. **Imbalance Issue**:\n",
    "   - The model still faces an imbalance issue, as seen in the lower precision for defaulting clients and the disparity between class performances.\n",
    "\n",
    "4. **Need for Improvement**:\n",
    "   - Although the model has improved, there is still room for enhancement, particularly in reducing false positives for defaulting clients.\n",
    "\n",
    "### Recommendations\n",
    "1. **Reassess the Model**:\n",
    "   - Consider fine-tuning the threshold for predicting defaulting clients to balance precision and recall better.\n",
    "   - Explore ensemble methods like balanced random forests or XGBoost with tailored class weights to further address class imbalance.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Further investigate and refine feature engineering to enhance model performance. Creating more relevant features can help the model differentiate better between classes.\n",
    "\n",
    "3. **Evaluate Data**:\n",
    "   - Ensure data quality and consider additional preprocessing steps, such as handling outliers and scaling features appropriately, to improve model performance.\n",
    "\n",
    "4. **Post-Processing**:\n",
    "   - Implement post-processing steps like cost-sensitive learning to account for the higher cost of false negatives, which could further improve decision-making.\n",
    "\n",
    "By continuing to refine the handling of class imbalance, feature engineering, and model evaluation, the model's ability to predict both defaulting and non-defaulting clients can be significantly enhanced."
   ],
   "id": "8e96b1ab04fa3037"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4.Feature Importance",
   "id": "898a240d06f7cede"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Global Feature Importance Using SHAP",
   "id": "b834e8e4483a1c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Explainer**: Create a SHAP explainer object for the best model's classifier using the training data.\n",
    "2. **Compute SHAP Values**: Generate SHAP values for the test dataset to explain the model's predictions.\n",
    "3. **Global Feature Importance**: Visualize the global feature importance using SHAP summary plot, which provides insights into how each feature contributes to the model's predictions."
   ],
   "id": "d4be0141f61189d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T14:23:33.957738Z",
     "start_time": "2024-06-17T14:22:48.306853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap  # Import the SHAP library used for interpreting model predictions\n",
    "import pandas as pd  # Import pandas for data manipulation\n",
    "\n",
    "# Convert the data to DataFrame if it's not already one\n",
    "X_train_df = pd.DataFrame(X_train)  # Convert the training data to DataFrame\n",
    "X_val_df = pd.DataFrame(X_val_split)  # Convert the validation data to DataFrame\n",
    "X_test_df = pd.DataFrame(X_test)  # Convert the test data to DataFrame\n",
    "\n",
    "# Function to reduce memory usage by sampling data\n",
    "def reduce_memory_usage(df, sample_fraction=0.1):\n",
    "    \"\"\"Reduces memory usage by sampling a fraction of the data.\"\"\"\n",
    "    return df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Reduce memory usage by sampling\n",
    "X_train_sampled = reduce_memory_usage(X_train_df)\n",
    "X_val_sampled = reduce_memory_usage(X_val_df)\n",
    "X_test_sampled = reduce_memory_usage(X_test_df)\n",
    "\n",
    "# Check and convert categorical variables to numeric\n",
    "X_train_sampled = pd.get_dummies(X_train_sampled)\n",
    "X_val_sampled = pd.get_dummies(X_val_sampled)\n",
    "X_test_sampled = pd.get_dummies(X_test_sampled)\n",
    "\n",
    "# Align the training, validation, and testing data to have the same columns\n",
    "X_train_sampled, X_val_sampled = X_train_sampled.align(X_val_sampled, join='inner', axis=1)\n",
    "X_train_sampled, X_test_sampled = X_train_sampled.align(X_test_sampled, join='inner', axis=1)\n",
    "X_val_sampled, X_test_sampled = X_val_sampled.align(X_test_sampled, join='inner', axis=1)\n",
    "\n",
    "# Ensure the columns are in the same order\n",
    "X_val_sampled = X_val_sampled[X_train_sampled.columns]\n",
    "X_test_sampled = X_test_sampled[X_train_sampled.columns]\n",
    "\n",
    "# Fit the explainer on the training data\n",
    "explainer = shap.Explainer(best_model.named_steps['classifier'], X_train_sampled)\n",
    "shap_values = explainer(X_test_sampled)  # Compute SHAP values for the test data\n",
    "\n",
    "# Global feature importance\n",
    "shap.summary_plot(shap_values, X_test_sampled)  # Visualize the global feature importance using SHAP summary plot\n",
    "\n",
    "# Local explanation for a single instance\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_test_sampled.iloc[0, :])\n"
   ],
   "id": "82a2346cb5b384ef",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (247,) and (4108,) not aligned: 247 (dim 0) != 4108 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 34>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     31\u001B[0m X_test_sampled \u001B[38;5;241m=\u001B[39m X_test_sampled[X_train_sampled\u001B[38;5;241m.\u001B[39mcolumns]\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Fit the explainer on the training data\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m explainer \u001B[38;5;241m=\u001B[39m \u001B[43mshap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mExplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnamed_steps\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclassifier\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_sampled\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m explainer(X_test_sampled)  \u001B[38;5;66;03m# Compute SHAP values for the test data\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# Global feature importance\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_explainer.py:191\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m \u001B[38;5;241m=\u001B[39m explainers\u001B[38;5;241m.\u001B[39mLinear\n\u001B[1;32m--> 191\u001B[0m     explainers\u001B[38;5;241m.\u001B[39mLinear\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasker, link\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlink, feature_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_names, linearize_link\u001B[38;5;241m=\u001B[39mlinearize_link, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeep\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m \u001B[38;5;241m=\u001B[39m explainers\u001B[38;5;241m.\u001B[39mDeep\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_linear.py:153\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[1;34m(self, model, masker, link, nsamples, feature_perturbation, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpected_value \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpected_value)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpected_value \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;66;03m# if needed, estimate the transform matrices\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (247,) and (4108,) not aligned: 247 (dim 0) != 4108 (dim 0)"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import shap  # Import the SHAP library used for interpreting model predictions\n",
    "# import pandas as pd  # Import pandas for data manipulation\n",
    "# \n",
    "# # Convert the data to a DataFrame if it's not already one\n",
    "# X_train_split_df = pd.DataFrame(X_train_split)  # Convert the training data to a DataFrame\n",
    "# X_val_split_df = pd.DataFrame(X_val_split)  # Convert the validation data to a DataFrame\n",
    "# X_test_df = pd.DataFrame(X_test)  # Convert the test data to a DataFrame\n",
    "# \n",
    "# # Function to reduce memory usage by sampling data\n",
    "# def reduce_memory_usage(df, sample_fraction=0.1):  # Reduce memory usage by sampling to a fraction of the data\n",
    "#     return df.sample(frac=sample_fraction, random_state=42)  # Sample a fraction of the data\n",
    "# \n",
    "# # Reduce memory usage by sampling\n",
    "# X_train_split_df_sampled = reduce_memory_usage(X_train_split_df)  # Sample the training data\n",
    "# X_val_split_df_sampled = reduce_memory_usage(X_val_split_df)  # Sample the validation data\n",
    "# X_test_df_sampled = reduce_memory_usage(X_test_df)  # Sample the test data\n",
    "# \n",
    "# # Check and convert categorical variables to numeric\n",
    "# X_train_split_df_sampled = pd.get_dummies(X_train_split_df_sampled)  # Convert categorical variables to numeric for training data\n",
    "# X_val_split_df_sampled = pd.get_dummies(X_val_split_df_sampled)  # Convert categorical variables to numeric for validation data\n",
    "# X_test_df_sampled = pd.get_dummies(X_test_df_sampled)  # Convert categorical variables to numeric for test data\n",
    "# \n",
    "# # Align the training, validation, and testing data to have the same columns\n",
    "# X_train_split_df_sampled, X_val_split_df_sampled = X_train_split_df_sampled.align(X_val_split_df_sampled, join='inner', axis=1)  # Align the training and validation data\n",
    "# X_train_split_df_sampled, X_test_df_sampled = X_train_split_df_sampled.align(X_test_df_sampled, join='inner', axis=1)  # Align the training and test data\n",
    "# X_val_split_df_sampled, X_test_df_sampled = X_val_split_df_sampled.align(X_test_df_sampled, join='inner', axis=1)  # Align the validation and test data\n",
    "# \n",
    "# # Ensure the columns are in the same order\n",
    "# X_val_split_df_sampled = X_val_split_df_sampled[X_train_split_df_sampled.columns]  # Ensure the columns are in the same order for validation data\n",
    "# X_test_df_sampled = X_test_df_sampled[X_train_split_df_sampled.columns]  # Ensure the columns are in the same order for test data\n",
    "# \n",
    "# # Fit the explainer on the training data\n",
    "# explainer = shap.Explainer(best_model.named_steps['classifier'], X_train_split_df_sampled)  # Fit the SHAP explainer on the training data\n",
    "# shap_values = explainer(X_val_split_df_sampled)  # Compute SHAP values for the validation data\n",
    "# \n",
    "# # Global feature importance\n",
    "# shap.summary_plot(shap_values, X_val_split_df_sampled)  # Visualize the global feature importance using SHAP summary plot\n",
    "# \n",
    "# # Local explanation for a single instance\n",
    "# shap.initjs()  # Initialize the SHAP JavaScript visualization\n",
    "# shap.force_plot(explainer.expected_value, shap_values[0, :], X_val_split_df_sampled.iloc[0, :])  # Generate a SHAP force plot for a single instance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ecd18ea859ae31f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Local Feature Importance Using SHAP",
   "id": "f06f82d571f049c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Visualization**: Set up SHAP's JavaScript visualization framework to enable interactive plots.\n",
    "2. **Generate Local Explanation**: Create a SHAP force plot for a single instance from the test dataset, showing how each feature contributes to the model's prediction for that specific instance.\n",
    "3. **Visualize Prediction Breakdown**: Provide a detailed breakdown of the prediction for the selected instance, illustrating the impact of each feature on the predicted value."
   ],
   "id": "187a45cda5219c68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Glossary\n",
    "\n",
    "### Cross-Validation\n",
    "A technique used to assess the performance of a model by splitting the dataset into multiple training and testing sets. This helps in understanding how the model will generalize to an independent dataset.\n",
    "\n",
    "### GridSearchCV\n",
    "A tool from `scikit-learn` that performs hyperparameter tuning by exhaustively searching through a specified parameter grid to find the best combination of hyperparameters for a given model, using cross-validation.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "The process of finding the optimal values for hyperparameters of a model, which are parameters that are not learned from data but set before the learning process begins. Examples include the number of trees in a random forest or the regularization strength in Ridge regression.\n",
    "\n",
    "### ROC AUC Score\n",
    "A performance measurement for classification problems at various threshold settings. ROC AUC represents the area under the Receiver Operating Characteristic curve, which plots the true positive rate against the false positive rate. A score closer to 1 indicates better performance.\n",
    "\n",
    "### Confusion Matrix\n",
    "A table used to describe the performance of a classification model by showing the actual vs. predicted classifications. It includes True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "### Classification Report\n",
    "A detailed report showing the precision, recall, F1-score, and support for each class in a classification problem. This helps in understanding the performance of the model across different classes.\n",
    "\n",
    "### Resampling Techniques\n",
    "Methods used to adjust the class distribution of a dataset, commonly used to handle class imbalance. Examples include:\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic samples for the minority class.\n",
    "- **RandomUnderSampler**: Reduces the number of samples in the majority class.\n",
    "\n",
    "### Pipeline\n",
    "A tool from `scikit-learn` that allows for chaining multiple processing steps (e.g., data transformation and model fitting) into a single object. This ensures that all steps are applied consistently during both training and testing.\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations)\n",
    "A method to explain individual predictions of machine learning models by assigning each feature an importance value. It helps in understanding how the model arrives at its predictions.\n",
    "\n",
    "### SHAP Explainer\n",
    "An object in the SHAP library that is used to calculate SHAP values for a given model. It helps in interpreting the contributions of each feature to the predictions.\n",
    "\n",
    "### SHAP Values\n",
    "Values calculated by the SHAP explainer that quantify the contribution of each feature to the model's prediction for a given instance. Higher absolute values indicate greater impact on the prediction.\n",
    "\n",
    "### SHAP Summary Plot\n",
    "A plot that visualizes the global importance of features by showing the distribution of SHAP values for each feature across all instances in the dataset. It helps in understanding which features are most influential for the model.\n",
    "\n",
    "### SHAP Force Plot\n",
    "A plot that provides a detailed breakdown of the contributions of each feature to a single instance's prediction, illustrating how different features push the prediction towards or away from the base value.\n",
    "\n",
    "### Random Forest\n",
    "An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "### Ridge Classifier\n",
    "A linear model for classification that includes L2 regularization (Ridge regression) to prevent overfitting by penalizing large coefficients.\n",
    "\n",
    "### Class Weight\n",
    "A parameter used to handle class imbalance by assigning different weights to different classes, typically to give more importance to the minority class. This can be set to 'balanced' to automatically adjust weights inversely proportional to class frequencies.\n",
    "\n",
    "### RandomUnderSampler\n",
    "A resampling technique that reduces the number of instances in the majority class by randomly sampling without replacement, used to balance the class distribution.\n",
    "\n",
    "### SMOTE\n",
    "A resampling technique that generates synthetic samples for the minority class by interpolating between existing minority class instances. It is used to balance the class distribution in the dataset.\n"
   ],
   "id": "27de101d1c04a5e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
