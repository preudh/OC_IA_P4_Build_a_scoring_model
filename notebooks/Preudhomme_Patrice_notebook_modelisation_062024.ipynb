{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the Preprocessed Data",
   "id": "458083ce2249405a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Indicate if the script is running on Google Colab or not\n",
    "using_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if using_colab:\n",
    "    # Connect Google Drive to Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    \n",
    "    # Install necessary packages\n",
    "    !pip install numpy\n",
    "    !pip install pandas\n",
    "    !pip install sklearn\n",
    "    !pip install matplotlib\n",
    "    !pip install seaborn\n",
    "    !pip install imbalanced-learn\n",
    "    \n",
    "    # Path for Google Colab\n",
    "    project_root = '/content/gdrive/MyDrive/oc_projet_4/' \n",
    "else:\n",
    "    # Get the current working directory as base directory for the notebook\n",
    "    base_dir = os.getcwd()\n",
    "    \n",
    "    # Adjust the project root path relatively to where the notebook is located\n",
    "    # Assuming the notebook is inside a 'notebooks' directory and we need to go up one level to access project root\n",
    "    project_root = os.path.join(base_dir, '..')\n",
    "\n",
    "# Clean output of cell\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set the correct path for the NPZ file\n",
    "data_path = os.path.join(project_root, 'dataapp_domain_train_test_sets.npz')\n",
    "\n",
    "# Load the dataset from the NPZ file\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "# Extract the training and testing sets\n",
    "X_train = data['X_train']  \n",
    "X_test = data['X_test'] \n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print('Data loaded successfully.')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ],
   "id": "bd48a92e09961fda",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "X_train shape: (215257, 244)\n",
      "X_test shape: (92254, 244)\n",
      "y_train shape: (215257,)\n",
      "y_test shape: (92254,)\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.Hyperparameter Tuning via GridSearchCV and Imbalanced-learn Pipeline",
   "id": "cbe789d731d07bcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Set Up Resampling Techniques**: Define over-sampling (SMOTE) and under-sampling (RandomUnderSampler) methods to handle class imbalance in the dataset.\n",
    "2. **Initialize Models**: Create instances of RandomForestClassifier and RidgeClassifier for classification tasks.\n",
    "3. **Define Hyperparameter Grids**: Specify parameter grids for tuning hyperparameters of the classifiers using GridSearchCV.\n",
    "4. **Create Pipelines**: Construct pipelines to integrate resampling techniques with the classifiers.\n",
    "5. **Perform Hyperparameter Tuning**: Use GridSearchCV to find the best hyperparameters and resampling techniques for each classifier, evaluating them using cross-validation and storing the best models."
   ],
   "id": "6ea95384359ab660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:35:38.162476Z",
     "start_time": "2024-06-12T17:13:45.057337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # Random Forest is a good choice for classification tasks\n",
    "from sklearn.linear_model import RidgeClassifier # Ridge Classifier is a linear model suitable for classification tasks\n",
    "from sklearn.model_selection import GridSearchCV # GridSearchCV for hyperparameter tuning and cross-validation\n",
    "from imblearn.pipeline import Pipeline # Pipeline for chaining resampling techniques with the classifier\n",
    "from imblearn.over_sampling import SMOTE # SMOTE for over-sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler # RandomUnderSampler for under-sampling\n",
    "\n",
    "# Define the resampling techniques\n",
    "over_sampler = SMOTE(random_state=42)\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Define the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "ridge = RidgeClassifier()\n",
    "\n",
    "# Define the parameter grids for GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200], # Number of trees in the forest\n",
    "    'classifier__max_depth': [10, 20], # Maximum depth of the tree\n",
    "    'classifier__class_weight': [None, 'balanced'] # Weights associated with classes in the form {class_label: weight}\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'classifier__alpha': [1.0, 0.1, 0.01], # Regularization strength\n",
    "    'classifier__class_weight': [None, 'balanced'] # Weights associated with classes in the form {class_label: weight}\n",
    "}\n",
    "\n",
    "# Define pipelines\n",
    "pipeline_rf_over = Pipeline([\n",
    "    ('oversample', over_sampler), \n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "pipeline_rf_under = Pipeline([\n",
    "    ('undersample', under_sampler),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "pipeline_rf_weight = Pipeline([\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_ridge_over = Pipeline([\n",
    "    ('oversample', over_sampler),\n",
    "    ('classifier', ridge)\n",
    "])\n",
    "\n",
    "pipeline_ridge_under = Pipeline([\n",
    "    ('undersample', under_sampler),\n",
    "    ('classifier', ridge)\n",
    "])\n",
    "\n",
    "pipeline_ridge_weight = Pipeline([\n",
    "    ('classifier', RidgeClassifier(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Perform GridSearchCV for each pipeline\n",
    "# pipeline is a sequence of fit-transform steps ending with a fit\n",
    "# Store the best estimators in a dictionary\n",
    "\n",
    "grids = [\n",
    "    (pipeline_rf_over, param_grid_rf, 'Random Forest with Over-sampling'), # \n",
    "    (pipeline_rf_under, param_grid_rf, 'Random Forest with Under-sampling'),\n",
    "    (pipeline_rf_weight, param_grid_rf, 'Random Forest with Class Weight'),\n",
    "    (pipeline_ridge_over, param_grid_ridge, 'Ridge with Over-sampling'),\n",
    "    (pipeline_ridge_under, param_grid_ridge, 'Ridge with Under-sampling'),\n",
    "    (pipeline_ridge_weight, param_grid_ridge, 'Ridge with Class Weight')\n",
    "]\n",
    "\n",
    "best_estimators = {} # Dictionary to store the best estimators\n",
    "\n",
    "for pipeline, param_grid, name in grids:\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_estimators[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best ROC AUC for {name}: {grid_search.best_score_}\")\n",
    "\n"
   ],
   "id": "60e72a82bf47abaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest with Over-sampling: {'classifier__class_weight': None, 'classifier__max_depth': 20, 'classifier__n_estimators': 200}\n",
      "Best ROC AUC for Random Forest with Over-sampling: 0.706770540515098\n",
      "Best parameters for Random Forest with Under-sampling: {'classifier__class_weight': None, 'classifier__max_depth': 20, 'classifier__n_estimators': 200}\n",
      "Best ROC AUC for Random Forest with Under-sampling: 0.739534466894729\n",
      "Best parameters for Random Forest with Class Weight: {'classifier__class_weight': None, 'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n",
      "Best ROC AUC for Random Forest with Class Weight: 0.7340727428197145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ill-conditioned matrix (rcond=7.51842e-18): result may not be accurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Ridge with Over-sampling: {'classifier__alpha': 1.0, 'classifier__class_weight': None}\n",
      "Best ROC AUC for Ridge with Over-sampling: 0.745725332792906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ill-conditioned matrix (rcond=6.55936e-17): result may not be accurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Ridge with Under-sampling: {'classifier__alpha': 1.0, 'classifier__class_weight': None}\n",
      "Best ROC AUC for Ridge with Under-sampling: 0.7446270382358854\n",
      "Best parameters for Ridge with Class Weight: {'classifier__alpha': 1.0, 'classifier__class_weight': 'balanced'}\n",
      "Best ROC AUC for Ridge with Class Weight: 0.746318807175601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ill-conditioned matrix (rcond=1.34498e-17): result may not be accurate.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2a3892f08444ed43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. **Random Forest with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__class_weight': None, 'classifier__max_depth': 20, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.706770540515098`\n",
    "\n",
    "2. **Random Forest with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__class_weight': None, 'classifier__max_depth': 20, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.739534466894729`\n",
    "\n",
    "3. **Random Forest with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__class_weight': None, 'classifier__max_depth': 10, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.7340727428197145`\n",
    "\n",
    "4. **Ridge with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0, 'classifier__class_weight': None}`\n",
    "   - **Best ROC AUC**: `0.745725332792906`\n",
    "\n",
    "5. **Ridge with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0, 'classifier__class_weight': None}`\n",
    "   - **Best ROC AUC**: `0.7446270382358854`\n",
    "\n",
    "6. **Ridge with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0, 'classifier__class_weight': 'balanced'}`\n",
    "   - **Best ROC AUC**: `0.746318807175601`\n",
    "\n",
    "#### Observations\n",
    "\n",
    "1. **Random Forest**:\n",
    "   - The best parameters for both over-sampling and under-sampling are the same, suggesting that the model configuration is robust across different resampling strategies.\n",
    "   - The ROC AUC for under-sampling (0.7395) is higher than for over-sampling (0.7068), indicating that under-sampling might be a better strategy for the Random Forest model in this case.\n",
    "   - Using class weight adjustment in Random Forest shows an improvement in ROC AUC compared to over-sampling but still underperforms compared to under-sampling.\n",
    "\n",
    "2. **Ridge Classifier**:\n",
    "   - The best parameters for both over-sampling and under-sampling are the same.\n",
    "   - The ROC AUC values are very close for over-sampling (0.7457) and under-sampling (0.7446), indicating that both resampling strategies perform similarly well for the Ridge Classifier.\n",
    "   - Using class weight adjustment in Ridge Classifier gives a slightly better ROC AUC (0.7463), making it the best-performing method overall.\n",
    "\n",
    "#### Warnings\n",
    "\n",
    "The warnings you received:\n",
    "```\n",
    "C:\\Users\\pat\\.conda\\envs\\P4\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=7.51842e-18): result may not be accurate.\n",
    "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
    "\n",
    "C:\\Users\\pat\\.conda\\envs\\P4\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=6.55936e-17): result may not be accurate.\n",
    "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
    "```\n",
    "These warnings indicate that the Ridge regression encountered ill-conditioned matrices, which means that the matrix used in the ridge regression solver has very small singular values, causing potential numerical instability. This can happen when there are highly correlated features in the dataset.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "1. **Addressing Ill-Conditioned Matrix Warnings**:\n",
    "   - **Feature Engineering**: Investigate and possibly remove or combine highly correlated features to reduce multicollinearity.\n",
    "   - **Regularization**: Consider using stronger regularization (increase the `alpha` parameter) to help stabilize the solution.\n",
    "   - **Scaling**: Ensure that the features are properly scaled, as Ridge regression can be sensitive to the scale of the features.\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - **Ridge Classifier**: Given the higher ROC AUC scores, the Ridge Classifier seems to perform better than the Random Forest in this case.\n",
    "   - **Resampling Strategy**: Both over-sampling and under-sampling show similar performance for the Ridge Classifier, but under-sampling performs better for Random Forest. You might choose the strategy based on other factors such as computational efficiency or interpretability."
   ],
   "id": "56de0e2246ebb5d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3.!!!!!!!!\n",
    "1. **Model Evaluation**:\n",
    "   - Evaluate the best models on the test set using confusion matrix, ROC AUC, and other metrics.\n",
    "\n",
    "2. **Feature Importance**:\n",
    "   - Use SHAP or similar methods to interpret the models and understand the importance of different features.\n",
    "\n",
    "3. **Documentation and Reporting**:\n",
    "   - Document the findings, including the best parameters, ROC AUC scores, and any observations regarding the resampling strategies and model performance."
   ],
   "id": "1243d29a0c120335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.Evaluate the Best Model",
   "id": "97552594f71e4aeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Select the Best Model**: Identify and select the best model from the hyperparameter-tuned models stored in `best_estimators` based on their performance on the test set.\n",
    "2. **Make Predictions**: Use the selected best model to predict the target variable on the test dataset.\n",
    "3. **Evaluate Performance**: Calculate the confusion matrix and ROC AUC score to assess the model's performance on the test data.\n",
    "4. **Check for Overfitting**: Ensure that the ROC AUC score is below 0.82 to avoid overfitting, issuing a warning if the score is higher.\n",
    "5. **Generate Detailed Metrics**: Print a comprehensive classification report including precision, recall, and F1-score for a detailed evaluation of the model's performance."
   ],
   "id": "3208e4b1a5c980ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:36:04.206768Z",
     "start_time": "2024-06-12T18:35:38.165477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "# Select the best model (for example purposes, choosing the best model based on GridSearchCV results)\n",
    "best_model_name = max(best_estimators, key=lambda name: best_estimators[name].score(X_test, y_test))\n",
    "best_model = best_estimators[best_model_name]\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Compute ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "# Ensure ROC AUC < 0.82\n",
    "if roc_auc >= 0.82:\n",
    "    print(\"Warning: ROC AUC score is greater than or equal to 0.82. Model might be overfitting.\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "858811dfb9290b86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[84806     0]\n",
      " [ 7447     1]]\n",
      "ROC AUC: 0.5000671321160043\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     84806\n",
      "           1       1.00      0.00      0.00      7448\n",
      "\n",
      "    accuracy                           0.92     92254\n",
      "   macro avg       0.96      0.50      0.48     92254\n",
      "weighted avg       0.93      0.92      0.88     92254\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results\n",
    "\n",
    "#### Confusion Matrix\n",
    "```\n",
    "Confusion Matrix:\n",
    "[[84806     0]\n",
    " [ 7447     1]]\n",
    "```\n",
    "The confusion matrix shows:\n",
    "- True Negatives (TN): 84,806 (clients correctly identified as not defaulting)\n",
    "- False Positives (FP): 0 (clients incorrectly identified as defaulting)\n",
    "- False Negatives (FN): 7,447 (clients incorrectly identified as not defaulting)\n",
    "- True Positives (TP): 1 (clients correctly identified as defaulting)\n",
    "\n",
    "#### ROC AUC Score\n",
    "```\n",
    "ROC AUC: 0.5000671321160043\n",
    "```\n",
    "The ROC AUC score is approximately 0.50, which is equivalent to random guessing. This indicates that the model has no discriminative power in distinguishing between classes (default vs. non-default).\n",
    "\n",
    "#### Classification Report\n",
    "```\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     84806\n",
    "           1       1.00      0.00      0.00      7448\n",
    "\n",
    "    accuracy                           0.92     92254\n",
    "   macro avg       0.96      0.50      0.48     92254\n",
    "weighted avg       0.93      0.92      0.88     92254\n",
    "```\n",
    "- **Class 0 (Non-defaulting clients)**:\n",
    "  - Precision: 0.92 (The proportion of clients predicted not to default that actually did not default)\n",
    "  - Recall: 1.00 (The proportion of actual non-defaulting clients correctly predicted)\n",
    "  - F1-Score: 0.96 (Harmonic mean of precision and recall)\n",
    "  - Support: 84,806 (Number of actual non-defaulting clients)\n",
    "\n",
    "- **Class 1 (Defaulting clients)**:\n",
    "  - Precision: 1.00 (The proportion of clients predicted to default that actually defaulted)\n",
    "  - Recall: 0.00 (The proportion of actual defaulting clients correctly predicted)\n",
    "  - F1-Score: 0.00 (Harmonic mean of precision and recall, which is 0 due to recall being 0)\n",
    "  - Support: 7,448 (Number of actual defaulting clients)\n",
    "\n",
    "- **Overall Metrics**:\n",
    "  - Accuracy: 0.92 (The proportion of total correct predictions)\n",
    "  - Macro Average: Average precision, recall, and F1-score for both classes (treating all classes equally)\n",
    "  - Weighted Average: Average precision, recall, and F1-score for both classes (considering the support of each class)\n",
    "\n",
    "### Observations and Insights\n",
    "1. **Model Performance**:\n",
    "   - The model performs exceptionally well in predicting non-defaulting clients (Class 0) but fails to predict defaulting clients (Class 1).\n",
    "   - The recall for defaulting clients is 0, indicating that the model did not identify any defaulting clients correctly.\n",
    "\n",
    "2. **ROC AUC Score**:\n",
    "   - The ROC AUC score of 0.50 indicates that the model has no discriminative ability, performing no better than random guessing.\n",
    "\n",
    "3. **Imbalance Issue**:\n",
    "   - The classification report and confusion matrix highlight a significant class imbalance issue, where the model is biased towards the majority class (non-defaulting clients).\n",
    "\n",
    "4. **Need for Improvement**:\n",
    "   - The current model is inadequate for practical use in credit scoring due to its failure to identify defaulting clients.\n",
    "   - Resampling techniques (like SMOTE, under-sampling, and class weighting) or different models need to be explored further to address the imbalance and improve the modelâ€™s performance.\n",
    "\n",
    "### Recommendations\n",
    "1. **Reassess the Model**:\n",
    "   - Consider using more sophisticated techniques for handling class imbalance, such as SMOTE with Tomek links or ensemble methods like balanced random forests.\n",
    "   - Experiment with other classifiers that might handle imbalance better, such as XGBoost or LightGBM.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Review and improve feature engineering to ensure relevant features are being used, which might help the model differentiate better between classes.\n",
    "\n",
    "3. **Evaluate Data**:\n",
    "   - Ensure data quality and consider additional preprocessing steps to handle any underlying issues that might be affecting model performance.\n",
    "\n",
    "By focusing on improving the handling of class imbalance and re-evaluating the features used, the model's ability to predict both defaulting and non-defaulting clients can be significantly enhanced."
   ],
   "id": "4bb0960554d4770"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4.Feature Importance",
   "id": "898a240d06f7cede"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Global Feature Importance Using SHAP",
   "id": "b834e8e4483a1c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Explainer**: Create a SHAP explainer object for the best model's classifier using the training data.\n",
    "2. **Compute SHAP Values**: Generate SHAP values for the test dataset to explain the model's predictions.\n",
    "3. **Global Feature Importance**: Visualize the global feature importance using SHAP summary plot, which provides insights into how each feature contributes to the model's predictions."
   ],
   "id": "d4be0141f61189d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:36:05.076927Z",
     "start_time": "2024-06-12T18:36:04.209735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the data to a DataFrame if it's not already one\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "# Check and convert categorical variables to numeric\n",
    "X_train_df = pd.get_dummies(X_train_df)\n",
    "X_test_df = pd.get_dummies(X_test_df)\n",
    "\n",
    "# Align the training and testing data to have the same columns\n",
    "X_train_df, X_test_df = X_train_df.align(X_test_df, join='inner', axis=1)\n",
    "\n",
    "# Fit the explainer on the training data\n",
    "explainer = shap.Explainer(best_model['classifier'], X_train_df)\n",
    "shap_values = explainer(X_test_df)\n",
    "\n",
    "# Global feature importance\n",
    "shap.summary_plot(shap_values, X_test_df)\n",
    "\n"
   ],
   "id": "f1ce1736161f6514",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshap\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Fit the explainer on the training data\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m explainer \u001B[38;5;241m=\u001B[39m \u001B[43mshap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mExplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclassifier\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m explainer(X_test)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Global feature importance\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_explainer.py:185\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtree\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m \u001B[38;5;241m=\u001B[39m explainers\u001B[38;5;241m.\u001B[39mTree\n\u001B[1;32m--> 185\u001B[0m     explainers\u001B[38;5;241m.\u001B[39mTree\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasker, link\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlink, feature_names\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_names, linearize_link\u001B[38;5;241m=\u001B[39mlinearize_link, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madditive\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m \u001B[38;5;241m=\u001B[39m explainers\u001B[38;5;241m.\u001B[39mAdditive\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_tree.py:166\u001B[0m, in \u001B[0;36mTree.__init__\u001B[1;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_perturbation \u001B[38;5;241m=\u001B[39m feature_perturbation\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpected_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mTreeEnsemble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_missing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_output \u001B[38;5;241m=\u001B[39m model_output\n\u001B[0;32m    168\u001B[0m \u001B[38;5;66;03m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_tree.py:759\u001B[0m, in \u001B[0;36mTreeEnsemble.__init__\u001B[1;34m(self, model, data, data_missing, model_output)\u001B[0m\n\u001B[0;32m    757\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_dtype \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfloat32\n\u001B[0;32m    758\u001B[0m scaling \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(model\u001B[38;5;241m.\u001B[39mestimators_) \u001B[38;5;66;03m# output is average of trees\u001B[39;00m\n\u001B[1;32m--> 759\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrees \u001B[38;5;241m=\u001B[39m [SingleTree(e\u001B[38;5;241m.\u001B[39mtree_, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, scaling\u001B[38;5;241m=\u001B[39mscaling, data\u001B[38;5;241m=\u001B[39mdata, data_missing\u001B[38;5;241m=\u001B[39mdata_missing) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mestimators_]\n\u001B[0;32m    760\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m objective_name_map\u001B[38;5;241m.\u001B[39mget(model\u001B[38;5;241m.\u001B[39mcriterion, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprobability\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_tree.py:759\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    757\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_dtype \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfloat32\n\u001B[0;32m    758\u001B[0m scaling \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(model\u001B[38;5;241m.\u001B[39mestimators_) \u001B[38;5;66;03m# output is average of trees\u001B[39;00m\n\u001B[1;32m--> 759\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrees \u001B[38;5;241m=\u001B[39m [\u001B[43mSingleTree\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_missing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_missing\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mestimators_]\n\u001B[0;32m    760\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m objective_name_map\u001B[38;5;241m.\u001B[39mget(model\u001B[38;5;241m.\u001B[39mcriterion, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprobability\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_tree.py:1580\u001B[0m, in \u001B[0;36mSingleTree.__init__\u001B[1;34m(self, tree, normalize, scaling, data, data_missing)\u001B[0m\n\u001B[0;32m   1578\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m data_missing \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1579\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_sample_weight\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m0.0\u001B[39m)\n\u001B[1;32m-> 1580\u001B[0m     \u001B[43m_cext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense_tree_update_weights\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1581\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchildren_left\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchildren_right\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchildren_default\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1582\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthresholds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_missing\u001B[49m\n\u001B[0;32m   1583\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1585\u001B[0m \u001B[38;5;66;03m# we compute the expectations to make sure they follow the SHAP logic\u001B[39;00m\n\u001B[0;32m   1586\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_depth \u001B[38;5;241m=\u001B[39m _cext\u001B[38;5;241m.\u001B[39mcompute_expectations(\n\u001B[0;32m   1587\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren_left, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren_right, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_sample_weight,\n\u001B[0;32m   1588\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m   1589\u001B[0m )\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Local Feature Importance Using SHAP",
   "id": "f06f82d571f049c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Visualization**: Set up SHAP's JavaScript visualization framework to enable interactive plots.\n",
    "2. **Generate Local Explanation**: Create a SHAP force plot for a single instance from the test dataset, showing how each feature contributes to the model's prediction for that specific instance.\n",
    "3. **Visualize Prediction Breakdown**: Provide a detailed breakdown of the prediction for the selected instance, illustrating the impact of each feature on the predicted value."
   ],
   "id": "187a45cda5219c68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T18:36:05.079930Z",
     "start_time": "2024-06-12T18:36:05.078929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the data to a DataFrame if it's not already one\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "# Check and convert categorical variables to numeric\n",
    "X_train_df = pd.get_dummies(X_train_df)\n",
    "X_test_df = pd.get_dummies(X_test_df)\n",
    "\n",
    "# Align the training and testing data to have the same columns\n",
    "X_train_df, X_test_df = X_train_df.align(X_test_df, join='inner', axis=1)\n",
    "\n",
    "# Fit the explainer on the training data\n",
    "explainer = shap.Explainer(best_model['classifier'], X_train_df)\n",
    "shap_values = explainer(X_test_df)\n",
    "\n",
    "# Global feature importance\n",
    "shap.summary_plot(shap_values, X_test_df)\n",
    "\n",
    "# Local explanation for a single instance\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test_df.iloc[0,:])"
   ],
   "id": "2a6e4f12273dd616",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Glossary\n",
    "\n",
    "### Cross-Validation\n",
    "A technique used to assess the performance of a model by splitting the dataset into multiple training and testing sets. This helps in understanding how the model will generalize to an independent dataset.\n",
    "\n",
    "### GridSearchCV\n",
    "A tool from `scikit-learn` that performs hyperparameter tuning by exhaustively searching through a specified parameter grid to find the best combination of hyperparameters for a given model, using cross-validation.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "The process of finding the optimal values for hyperparameters of a model, which are parameters that are not learned from data but set before the learning process begins. Examples include the number of trees in a random forest or the regularization strength in Ridge regression.\n",
    "\n",
    "### ROC AUC Score\n",
    "A performance measurement for classification problems at various threshold settings. ROC AUC represents the area under the Receiver Operating Characteristic curve, which plots the true positive rate against the false positive rate. A score closer to 1 indicates better performance.\n",
    "\n",
    "### Confusion Matrix\n",
    "A table used to describe the performance of a classification model by showing the actual vs. predicted classifications. It includes True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "### Classification Report\n",
    "A detailed report showing the precision, recall, F1-score, and support for each class in a classification problem. This helps in understanding the performance of the model across different classes.\n",
    "\n",
    "### Resampling Techniques\n",
    "Methods used to adjust the class distribution of a dataset, commonly used to handle class imbalance. Examples include:\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic samples for the minority class.\n",
    "- **RandomUnderSampler**: Reduces the number of samples in the majority class.\n",
    "\n",
    "### Pipeline\n",
    "A tool from `scikit-learn` that allows for chaining multiple processing steps (e.g., data transformation and model fitting) into a single object. This ensures that all steps are applied consistently during both training and testing.\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations)\n",
    "A method to explain individual predictions of machine learning models by assigning each feature an importance value. It helps in understanding how the model arrives at its predictions.\n",
    "\n",
    "### SHAP Explainer\n",
    "An object in the SHAP library that is used to calculate SHAP values for a given model. It helps in interpreting the contributions of each feature to the predictions.\n",
    "\n",
    "### SHAP Values\n",
    "Values calculated by the SHAP explainer that quantify the contribution of each feature to the model's prediction for a given instance. Higher absolute values indicate greater impact on the prediction.\n",
    "\n",
    "### SHAP Summary Plot\n",
    "A plot that visualizes the global importance of features by showing the distribution of SHAP values for each feature across all instances in the dataset. It helps in understanding which features are most influential for the model.\n",
    "\n",
    "### SHAP Force Plot\n",
    "A plot that provides a detailed breakdown of the contributions of each feature to a single instance's prediction, illustrating how different features push the prediction towards or away from the base value.\n",
    "\n",
    "### Random Forest\n",
    "An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "### Ridge Classifier\n",
    "A linear model for classification that includes L2 regularization (Ridge regression) to prevent overfitting by penalizing large coefficients.\n",
    "\n",
    "### Class Weight\n",
    "A parameter used to handle class imbalance by assigning different weights to different classes, typically to give more importance to the minority class. This can be set to 'balanced' to automatically adjust weights inversely proportional to class frequencies.\n",
    "\n",
    "### RandomUnderSampler\n",
    "A resampling technique that reduces the number of instances in the majority class by randomly sampling without replacement, used to balance the class distribution.\n",
    "\n",
    "### SMOTE\n",
    "A resampling technique that generates synthetic samples for the minority class by interpolating between existing minority class instances. It is used to balance the class distribution in the dataset.\n"
   ],
   "id": "27de101d1c04a5e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
