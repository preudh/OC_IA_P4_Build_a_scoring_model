{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the Preprocessed Data",
   "id": "458083ce2249405a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Indicate if the script is running on Google Colab or not\n",
    "using_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if using_colab:\n",
    "    # Connect Google Drive to Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    \n",
    "    # Install necessary packages\n",
    "    !pip install numpy pandas scikit-learn matplotlib seaborn imbalanced-learn shap\n",
    "    \n",
    "    # Path for Google Colab\n",
    "    project_root = '/content/gdrive/MyDrive/oc_projet_4/' \n",
    "else:\n",
    "    # Get the current working directory as base directory for the notebook\n",
    "    base_dir = os.getcwd()\n",
    "    \n",
    "    # Adjust the project root path relatively to where the notebook is located\n",
    "    # Assuming the notebook is inside a 'notebooks' directory and we need to go up one level to access project root\n",
    "    project_root = os.path.join(base_dir, '..')\n",
    "\n",
    "# Clean output of cell\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ],
   "id": "afc42854a3e4780d",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T13:32:32.545519Z",
     "start_time": "2024-06-21T13:32:24.652914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import gc\n",
    "\n",
    "# Load the CSV file\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "\n",
    "app_train_domain = pd.read_csv(train_domain_path)  # Load the training data\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = app_train_domain.drop(columns=['TARGET']).values  # Features for training\n",
    "y = app_train_domain['TARGET'].values  # Target variable for training\n",
    "\n",
    "print('Data loaded successfully.')\n",
    "print(f'X shape: {X.shape}')  # X is used for training the model\n",
    "print(f'y shape: {y.shape}')  # y is the target variable for training"
   ],
   "id": "bd48a92e09961fda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "X shape: (307511, 247)\n",
      "y shape: (307511,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T13:32:39.195496Z",
     "start_time": "2024-06-21T13:32:32.547486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, zscore\n",
    "import warnings\n",
    "\n",
    "using_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if using_colab:\n",
    "    # Load the CSV file using the relative path\n",
    "    train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "else:\n",
    "    # Get the current working directory as base directory for the notebook\n",
    "    base_dir = os.getcwd()\n",
    "    \n",
    "    # Adjust the project root path relatively to where the notebook is located\n",
    "    # Assuming the notebook is inside a 'notebooks' directory and we need to go up one level to access project root\n",
    "    project_root = os.path.join(base_dir, '..')\n",
    "\n",
    "# Load the CSV file using relative path\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "\n",
    "train_data = pd.read_csv(train_domain_path)  # Load the training data\n",
    "\n",
    "# Features to analyze\n",
    "features_to_analyze = [\n",
    "    'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM',\n",
    "    'DAYS_EMPLOYED_PERCENT', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'\n",
    "]\n",
    "\n",
    "# Shapiro-Wilk Test for normality\n",
    "def test_normality(df, features):\n",
    "    normality_results = {}\n",
    "    warnings_list = []\n",
    "    for feature in features:\n",
    "        data = df[feature].dropna()\n",
    "        if len(data) > 5000:\n",
    "            warnings_list.append(f\"N = {len(data)} for feature {feature}\")\n",
    "        stat, p = shapiro(data)  # Ignore missing values for the test\n",
    "        normality_results[feature] = {'Statistic': stat, 'p-value': p, 'Normal': p > 0.05}\n",
    "    \n",
    "    if warnings_list:\n",
    "        warnings.warn(f\"For N > 5000, computed p-value may not be accurate. {', '.join(warnings_list)}\")\n",
    "    \n",
    "    return normality_results\n",
    "\n",
    "# Detecting outliers using z-score\n",
    "def detect_outliers(df, features):\n",
    "    outliers_results = {}\n",
    "    for feature in features:\n",
    "        z_scores = zscore(df[feature].dropna())  # Ignore missing values for z-score calculation\n",
    "        outliers = np.where(np.abs(z_scores) > 3)[0]  # Outliers with z-score > 3\n",
    "        outliers_results[feature] = df[feature].iloc[outliers]\n",
    "    return outliers_results\n",
    "\n",
    "# Execute the functions\n",
    "normality_results = test_normality(train_data, features_to_analyze)\n",
    "outliers_results = detect_outliers(train_data, features_to_analyze)\n",
    "\n",
    "# Display the results\n",
    "print(\"Normality Test Results (Shapiro-Wilk):\")\n",
    "for feature, result in normality_results.items():\n",
    "    print(f\"{feature}: Statistic={result['Statistic']}, p-value={result['p-value']}, Normal={result['Normal']}\")\n",
    "\n",
    "print(\"\\nDetected Outliers:\")\n",
    "for feature, outliers in outliers_results.items():\n",
    "    print(f\"{feature}: {len(outliers)} outliers\")\n",
    "    print(outliers)\n"
   ],
   "id": "41eb48546b60ab7a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pat\\AppData\\Local\\Temp\\ipykernel_23212\\2760031153.py:39: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 307511.\n",
      "  stat, p = shapiro(data)  # Ignore missing values for the test\n",
      "C:\\Users\\pat\\AppData\\Local\\Temp\\ipykernel_23212\\2760031153.py:43: UserWarning: For N > 5000, computed p-value may not be accurate. N = 307511 for feature CREDIT_INCOME_PERCENT, N = 307511 for feature ANNUITY_INCOME_PERCENT, N = 307511 for feature CREDIT_TERM, N = 307511 for feature DAYS_EMPLOYED_PERCENT, N = 307511 for feature EXT_SOURCE_1, N = 307511 for feature EXT_SOURCE_2, N = 307511 for feature EXT_SOURCE_3, N = 307511 for feature DAYS_BIRTH\n",
      "  warnings.warn(f\"For N > 5000, computed p-value may not be accurate. {', '.join(warnings_list)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normality Test Results (Shapiro-Wilk):\n",
      "CREDIT_INCOME_PERCENT: Statistic=0.8683387382469063, p-value=7.017665490743911e-144, Normal=False\n",
      "ANNUITY_INCOME_PERCENT: Statistic=0.9120356975923316, p-value=1.7391184523648002e-131, Normal=False\n",
      "CREDIT_TERM: Statistic=0.8813492261004188, p-value=1.2529197766384718e-140, Normal=False\n",
      "DAYS_EMPLOYED_PERCENT: Statistic=0.8455920689876675, p-value=6.2034854179349085e-149, Normal=False\n",
      "EXT_SOURCE_1: Statistic=0.8413734001601811, p-value=8.494600632668699e-150, Normal=False\n",
      "EXT_SOURCE_2: Statistic=0.9267468129817444, p-value=4.763031590479468e-126, Normal=False\n",
      "EXT_SOURCE_3: Statistic=0.9644152629779538, p-value=9.85564431307928e-106, Normal=False\n",
      "DAYS_BIRTH: Statistic=0.9696564518865638, p-value=1.709357664268864e-101, Normal=False\n",
      "\n",
      "Detected Outliers:\n",
      "CREDIT_INCOME_PERCENT: 4340 outliers\n",
      "176       12.835714\n",
      "270       13.754462\n",
      "371       13.611000\n",
      "380       12.112000\n",
      "678       34.916667\n",
      "            ...    \n",
      "307369    13.742400\n",
      "307401    12.729433\n",
      "307443    16.217176\n",
      "307476    13.052667\n",
      "307480    16.666667\n",
      "Name: CREDIT_INCOME_PERCENT, Length: 4340, dtype: float64\n",
      "ANNUITY_INCOME_PERCENT: 3792 outliers\n",
      "60        0.483275\n",
      "678       1.373917\n",
      "719       0.494467\n",
      "733       0.504385\n",
      "779       0.761000\n",
      "            ...   \n",
      "307247    0.486640\n",
      "307363    0.735692\n",
      "307405    0.533727\n",
      "307443    0.582471\n",
      "307480    0.487333\n",
      "Name: ANNUITY_INCOME_PERCENT, Length: 3792, dtype: float64\n",
      "CREDIT_TERM: 106 outliers\n",
      "7474      0.121201\n",
      "7619      0.121292\n",
      "7753      0.124429\n",
      "13055     0.124428\n",
      "15537     0.121201\n",
      "            ...   \n",
      "289178    0.124428\n",
      "297906    0.121500\n",
      "297961    0.121388\n",
      "300090    0.124414\n",
      "300382    0.121388\n",
      "Name: CREDIT_TERM, Length: 106, dtype: float64\n",
      "DAYS_EMPLOYED_PERCENT: 6975 outliers\n",
      "49       -0.521408\n",
      "95       -0.527185\n",
      "127      -0.529192\n",
      "249      -0.517723\n",
      "259      -0.625676\n",
      "            ...   \n",
      "307397   -0.531505\n",
      "307408   -0.529349\n",
      "307413   -0.591815\n",
      "307502   -0.543859\n",
      "307508   -0.529266\n",
      "Name: DAYS_EMPLOYED_PERCENT, Length: 6975, dtype: float64\n",
      "EXT_SOURCE_1: 1593 outliers\n",
      "0         0.083037\n",
      "128       0.053183\n",
      "297       0.068260\n",
      "415       0.084902\n",
      "470       0.040684\n",
      "            ...   \n",
      "306896    0.065634\n",
      "307027    0.081045\n",
      "307126    0.072247\n",
      "307338    0.932105\n",
      "307448    0.073452\n",
      "Name: EXT_SOURCE_1, Length: 1593, dtype: float64\n",
      "EXT_SOURCE_2: 0 outliers\n",
      "Series([], Name: EXT_SOURCE_2, dtype: float64)\n",
      "EXT_SOURCE_3: 0 outliers\n",
      "Series([], Name: EXT_SOURCE_3, dtype: float64)\n",
      "DAYS_BIRTH: 0 outliers\n",
      "Series([], Name: DAYS_BIRTH, dtype: int64)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Analysis and Interpretation\n",
    "\n",
    "#### Normality Test Results (Shapiro-Wilk)\n",
    "\n",
    "The Shapiro-Wilk test was applied to each feature to assess normality. Here are the results:\n",
    "\n",
    "- **CREDIT_INCOME_PERCENT**: Statistic=0.868, p-value=7.018e-144, Normal=False\n",
    "- **ANNUITY_INCOME_PERCENT**: Statistic=0.912, p-value=1.739e-131, Normal=False\n",
    "- **CREDIT_TERM**: Statistic=0.881, p-value=1.253e-140, Normal=False\n",
    "- **DAYS_EMPLOYED_PERCENT**: Statistic=0.846, p-value=6.203e-149, Normal=False\n",
    "- **EXT_SOURCE_1**: Statistic=0.841, p-value=8.495e-150, Normal=False\n",
    "- **EXT_SOURCE_2**: Statistic=0.927, p-value=4.763e-126, Normal=False\n",
    "- **EXT_SOURCE_3**: Statistic=0.964, p-value=9.856e-106, Normal=False\n",
    "- **DAYS_BIRTH**: Statistic=0.970, p-value=1.709e-101, Normal=False\n",
    "\n",
    "**Interpretation**:\n",
    "- All the p-values are significantly smaller than 0.05, which means that none of the features follow a normal distribution. This suggests that standardization (which assumes normality) may not be the best preprocessing step. Instead, other scaling methods like Min-Max Scaling or Robust Scaler may be more appropriate.\n",
    "\n",
    "#### Detected Outliers\n",
    "\n",
    "The z-score method was used to detect outliers in each feature. Here are the results:\n",
    "\n",
    "- **CREDIT_INCOME_PERCENT**: 4340 outliers\n",
    "- **ANNUITY_INCOME_PERCENT**: 3792 outliers\n",
    "- **CREDIT_TERM**: 106 outliers\n",
    "- **DAYS_EMPLOYED_PERCENT**: 6975 outliers\n",
    "- **EXT_SOURCE_1**: 1593 outliers\n",
    "- **EXT_SOURCE_2**: 0 outliers\n",
    "- **EXT_SOURCE_3**: 0 outliers\n",
    "- **DAYS_BIRTH**: 0 outliers\n",
    "\n",
    "**Interpretation**:\n",
    "- **CREDIT_INCOME_PERCENT**, **ANNUITY_INCOME_PERCENT**, **DAYS_EMPLOYED_PERCENT**, and **EXT_SOURCE_1** have a significant number of outliers. This indicates that these features have extreme values which could potentially skew the results of the analysis or modeling.\n",
    "- **EXT_SOURCE_2**, **EXT_SOURCE_3**, and **DAYS_BIRTH** do not have any outliers detected with the z-score method, suggesting these features are more stable and less prone to extreme values.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Given these findings, here are some recommendations for preprocessing and modeling:\n",
    "\n",
    "1. **Normalization/Scaling**:\n",
    "   - Since the features do not follow a normal distribution, consider using **Min-Max Scaling** or **Robust Scaler** instead of standardization. Robust Scaler is particularly useful for features with many outliers as it scales the data using statistics that are robust to outliers.\n",
    "\n",
    "2. **Handling Outliers**:\n",
    "   - For features with significant outliers, consider applying techniques such as:\n",
    "     - **Winsorizing**: Capping the extreme values.\n",
    "     - **Transformation**: Applying log, square root, or other transformations to reduce the effect of outliers.\n",
    "     - **Removal**: Removing data points that are identified as outliers, although this should be done cautiously to avoid losing valuable information.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Investigate if some of these outliers are due to data entry errors or other anomalies that can be corrected.\n",
    "   - For features with many outliers, you can create additional features that capture the presence of outliers, e.g., a binary feature indicating if a value is an outlier or not.\n",
    "\n",
    "4. **Modeling**:\n",
    "   - Use models that are less sensitive to outliers, such as tree-based methods (e.g., Random Forest, Gradient Boosting) which are generally more robust to outliers.\n",
    "   - Ensure that you have robust cross-validation practices to evaluate the impact of these preprocessing steps.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Implement Min-Max Scaling or Robust Scaling on the features.\n",
    "- Apply outlier handling techniques as needed.\n",
    "- Re-evaluate the performance of your model with these preprocessing steps.\n",
    "- Ensure that your evaluation metrics align with the business goals, such as minimizing the cost of prediction errors as outlined in your project requirements.\n",
    "\n",
    "By following these recommendations, you can ensure that your data preprocessing is robust and your models are well-prepared to handle the characteristics of your dataset."
   ],
   "id": "3168660548360e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.Hyperparameter Tuning via GridSearchCV and Imbalanced-learn Pipeline with make_scorer for Custom Cost Metric",
   "id": "cbe789d731d07bcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Set Up Resampling Techniques**: Define over-sampling (SMOTE) and under-sampling (RandomUnderSampler) methods to handle class imbalance in the dataset.\n",
    "2. **Initialize Models**: Create instances of RandomForestClassifier and RidgeClassifier for classification tasks.\n",
    "3. **Define Hyperparameter Grids**: Specify parameter grids for tuning hyperparameters of the classifiers using GridSearchCV.\n",
    "4. **Create Pipelines**: Construct pipelines to integrate resampling techniques with the classifiers.\n",
    "5. **Perform Hyperparameter Tuning**: Use GridSearchCV to find the best hyperparameters and resampling techniques for each classifier, evaluating them using cross-validation and storing the best models."
   ],
   "id": "6ea95384359ab660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:20:53.625702Z",
     "start_time": "2024-06-21T13:32:39.197497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, precision_recall_curve, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Custom cost metric function\n",
    "def cost_metric(y_true, y_pred):\n",
    "    false_negatives = ((y_true == 1) & (y_pred == 0)).sum()  # Count the false negatives\n",
    "    false_positives = ((y_true == 0) & (y_pred == 1)).sum()  # Count the false positives\n",
    "    return 10 * false_negatives + false_positives  # Return the cost metric\n",
    "\n",
    "# Create a scorer using the custom cost metric\n",
    "cost_scorer = make_scorer(cost_metric, greater_is_better=False)  # Setting greater_is_better to False because we want to minimize the cost\n",
    "\n",
    "# Load the data\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "app_train_domain = pd.read_csv(train_domain_path)  # Load the training data\n",
    "\n",
    "# Extract the features and target\n",
    "X = app_train_domain.drop(columns=['TARGET']).values  # Features\n",
    "y = app_train_domain['TARGET'].values  # Target variable\n",
    "\n",
    "# Define the resampling techniques\n",
    "over_sampler = SMOTE(random_state=42)  # Over-sampling method\n",
    "under_sampler = RandomUnderSampler(random_state=42)  # Under-sampling method\n",
    "\n",
    "# Define the models\n",
    "rf = RandomForestClassifier(random_state=42)  # Random Forest Classifier\n",
    "ridge = RidgeClassifier()  # Create an instance of the Ridge Classifier\n",
    "\n",
    "# Define the parameter grids for GridSearchCV with a larger hyperparameter space\n",
    "param_grid_rf = {  # Parameter grid for Random Forest\n",
    "    'classifier__n_estimators': [100, 200, 300, 400],  # Number of trees in the forest\n",
    "    'classifier__max_depth': [10, 20, 30, 40],  # Maximum depth of the tree\n",
    "}\n",
    "\n",
    "param_grid_ridge = {  # Parameter grid for Ridge Classifier\n",
    "    'classifier__alpha': [1.0, 0.1, 0.01, 0.001]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Define pipelines with Robust Scaling\n",
    "pipeline_rf_over = Pipeline([  # Pipeline for Random Forest with over-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('oversample', over_sampler),  # Over-sampling method\n",
    "    ('classifier', rf)  # Random Forest Classifier\n",
    "])\n",
    "\n",
    "pipeline_rf_under = Pipeline([  # Pipeline for Random Forest with under-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('undersample', under_sampler),  # Under-sampling method\n",
    "    ('classifier', rf)  # Random Forest Classifier\n",
    "])\n",
    "\n",
    "pipeline_rf_weight = Pipeline([  # Pipeline for Random Forest with class weight adjustment\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))  # Random Forest Classifier with class weight adjustment\n",
    "])\n",
    "\n",
    "pipeline_ridge_over = Pipeline([  # Pipeline for Ridge Classifier avec over-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('oversample', over_sampler),  # Over-sampling method\n",
    "    ('classifier', ridge)  # Ridge Classifier\n",
    "])\n",
    "\n",
    "pipeline_ridge_under = Pipeline([  # Pipeline for Ridge Classifier avec under-sampling\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('undersample', under_sampler),  # Under-sampling method\n",
    "    ('classifier', ridge)  # Ridge Classifier\n",
    "])\n",
    "\n",
    "pipeline_ridge_weight = Pipeline([  # Pipeline for Ridge Classifier avec class weight adjustment\n",
    "    ('scaler', RobustScaler()),  # Robust Scaling\n",
    "    ('classifier', RidgeClassifier(class_weight='balanced'))  # Ridge Classifier with class weight adjustment\n",
    "])\n",
    "\n",
    "# Perform GridSearchCV for each pipeline and store the best estimators\n",
    "grids = [\n",
    "    (pipeline_rf_over, param_grid_rf, 'Random Forest with Over-sampling'),  # pipeline, parameter grid, name\n",
    "    (pipeline_rf_under, param_grid_rf, 'Random Forest with Under-sampling'),\n",
    "    (pipeline_rf_weight, param_grid_rf, 'Random Forest with Class Weight'),\n",
    "    (pipeline_ridge_over, param_grid_ridge, 'Ridge with Over-sampling'),\n",
    "    (pipeline_ridge_under, param_grid_ridge, 'Ridge with Under-sampling'),\n",
    "    (pipeline_ridge_weight, param_grid_ridge, 'Ridge with Class Weight')\n",
    "]\n",
    "\n",
    "best_estimators = {}  # Dictionary to store the best estimators\n",
    "roc_auc_scores = {}  # Dictionary to store the ROC AUC scores for each model\n",
    "\n",
    "for pipeline, param_grid, name in grids:  # Iterate over the pipelines\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=cost_scorer, cv=3, n_jobs=-1)  # GridSearchCV for hyperparameter tuning\n",
    "    grid_search.fit(X, y)  # Fit the model for training data\n",
    "    best_estimators[name] = grid_search.best_estimator_  # Store the best estimator\n",
    "    \n",
    "    # Use cross_val_predict to get predictions for ROC AUC calculation\n",
    "    if hasattr(grid_search.best_estimator_['classifier'], 'predict_proba'):\n",
    "        y_pred = cross_val_predict(grid_search.best_estimator_, X, y, cv=3, method='predict_proba')[:, 1]\n",
    "    elif hasattr(grid_search.best_estimator_['classifier'], 'decision_function'):\n",
    "        y_pred = cross_val_predict(grid_search.best_estimator_, X, y, cv=3, method='decision_function')\n",
    "    else:\n",
    "        y_pred = cross_val_predict(grid_search.best_estimator_, X, y, cv=3, method='predict')\n",
    "        \n",
    "    roc_auc = roc_auc_score(y, y_pred)  # Compute ROC AUC\n",
    "    roc_auc_scores[name] = roc_auc  # Store the ROC AUC score\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")  # Print the best parameters\n",
    "    print(f\"Best ROC AUC for {name}: {roc_auc}\")  # Print the best ROC AUC score\n",
    "\n",
    "# Select the best model based on ROC AUC scores\n",
    "best_model_name = max(roc_auc_scores, key=roc_auc_scores.get)  # Key is a function that defines the sorting criteria\n",
    "best_model_final = best_estimators[best_model_name]  # Get the best model\n",
    "print(f\"\\nBest model: {best_model_name} with ROC AUC: {roc_auc_scores[best_model_name]}\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Partie inchangée pour l'évaluation sur l'ensemble de validation\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Split the data into training and validation sets for independent evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Predict the target variable on the validation set\n",
    "y_val_pred = best_model_final.predict(X_val)  # Predict the target variable on the validation set\n",
    "y_val_proba = best_model_final.predict_proba(X_val)[:, 1] if hasattr(best_model_final, \"predict_proba\") else best_model_final.decision_function(X_val) if hasattr(best_model_final, \"decision_function\") else best_model_final.predict(X_val)  # Get the predicted probabilities\n",
    "\n",
    "# Compute confusion matrix for validation set\n",
    "conf_matrix_val = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Confusion Matrix (Validation):\")\n",
    "print(conf_matrix_val)\n",
    "\n",
    "# Compute ROC AUC score for validation set using probabilities\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"ROC AUC (Validation): {roc_auc_val}\")\n",
    "\n",
    "# Ensure ROC AUC < 0.82 for validation set\n",
    "if roc_auc_val >= 0.82:  # Check if ROC AUC score is greater than or equal to 0.82 otherwise issue a warning; 0.82 is a threshold to avoid overfitting\n",
    "    print(\"Warning: ROC AUC score (Validation) is greater than or equal to 0.82. Model might be overfitting.\")\n",
    "\n",
    "# Print classification report for validation set\n",
    "print(\"Classification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print the custom cost metric\n",
    "validation_cost = cost_metric(y_val, y_val_pred)\n",
    "print(f\"Validation Cost: {validation_cost}\")\n",
    "\n",
    "# Optimiser le seuil de prédiction\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "optimal_idx = np.argmax(fscore)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "# Appliquer le seuil optimal pour les prédictions\n",
    "y_val_pred_optimal = (y_val_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Compute confusion matrix with optimal threshold\n",
    "conf_matrix_optimal = confusion_matrix(y_val, y_val_pred_optimal)\n",
    "print(\"Confusion Matrix (Validation) with Optimal Threshold:\")\n",
    "print(conf_matrix_optimal)\n",
    "\n",
    "# Print classification report with optimal threshold\n",
    "print(\"Classification Report (Validation) with Optimal Threshold:\")\n",
    "print(classification_report(y_val, y_val_pred_optimal))\n",
    "\n",
    "# Calculate and print the custom cost metric with optimal threshold\n",
    "validation_cost_optimal = cost_metric(y_val, y_val_pred_optimal)\n",
    "print(f\"Validation Cost with Optimal Threshold: {validation_cost_optimal}\")\n"
   ],
   "id": "8a5968c98b96b7b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest with Over-sampling: {'classifier__max_depth': 10, 'classifier__n_estimators': 100}\n",
      "Best ROC AUC for Random Forest with Over-sampling: 0.6789056950367194\n",
      "Best parameters for Random Forest with Under-sampling: {'classifier__max_depth': 40, 'classifier__n_estimators': 400}\n",
      "Best ROC AUC for Random Forest with Under-sampling: 0.7363337535790586\n",
      "Best parameters for Random Forest with Class Weight: {'classifier__max_depth': 10, 'classifier__n_estimators': 400}\n",
      "Best ROC AUC for Random Forest with Class Weight: 0.7256759933886698\n",
      "Best parameters for Ridge with Over-sampling: {'classifier__alpha': 1.0}\n",
      "Best ROC AUC for Ridge with Over-sampling: 0.7424425630866793\n",
      "Best parameters for Ridge with Under-sampling: {'classifier__alpha': 1.0}\n",
      "Best ROC AUC for Ridge with Under-sampling: 0.7457091113139179\n",
      "Best parameters for Ridge with Class Weight: {'classifier__alpha': 0.1}\n",
      "Best ROC AUC for Ridge with Class Weight: 0.7473402949930767\n",
      "\n",
      "Best model: Ridge with Class Weight with ROC AUC: 0.7473402949930767\n",
      "Confusion Matrix (Validation):\n",
      "[[39018 17536]\n",
      " [ 1580  3369]]\n",
      "ROC AUC (Validation): 0.7525304700583072\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.69      0.80     56554\n",
      "         1.0       0.16      0.68      0.26      4949\n",
      "\n",
      "    accuracy                           0.69     61503\n",
      "   macro avg       0.56      0.69      0.53     61503\n",
      "weighted avg       0.90      0.69      0.76     61503\n",
      "\n",
      "Validation Cost: 33336\n",
      "Optimal Threshold: 0.21968209052295357\n",
      "Confusion Matrix (Validation) with Optimal Threshold:\n",
      "[[48102  8452]\n",
      " [ 2602  2347]]\n",
      "Classification Report (Validation) with Optimal Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.85      0.90     56554\n",
      "         1.0       0.22      0.47      0.30      4949\n",
      "\n",
      "    accuracy                           0.82     61503\n",
      "   macro avg       0.58      0.66      0.60     61503\n",
      "weighted avg       0.89      0.82      0.85     61503\n",
      "\n",
      "Validation Cost with Optimal Threshold: 34472\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. **Random Forest with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__max_depth': 10, 'classifier__n_estimators': 100}`\n",
    "   - **Best ROC AUC**: `0.6789056950367194`\n",
    "\n",
    "2. **Random Forest with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__max_depth': 40, 'classifier__n_estimators': 400}`\n",
    "   - **Best ROC AUC**: `0.7363337535790586`\n",
    "\n",
    "3. **Random Forest with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__max_depth': 10, 'classifier__n_estimators': 400}`\n",
    "   - **Best ROC AUC**: `0.7256759933886698`\n",
    "\n",
    "4. **Ridge with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0}`\n",
    "   - **Best ROC AUC**: `0.7424425630866793`\n",
    "\n",
    "5. **Ridge with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0}`\n",
    "   - **Best ROC AUC**: `0.7457091113139179`\n",
    "\n",
    "6. **Ridge with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 0.1}`\n",
    "   - **Best ROC AUC**: `0.7473402949930767`\n",
    "\n",
    "#### Observations\n",
    "\n",
    "1. **Random Forest**:\n",
    "   - The best parameters for over-sampling and under-sampling are different, indicating that the model configuration adjusts based on the resampling strategy used.\n",
    "   - The ROC AUC for under-sampling (0.7363) is higher than for over-sampling (0.6789), suggesting that under-sampling might be a more effective strategy for the Random Forest model in this case.\n",
    "   - Using class weight adjustment in Random Forest shows an improvement in ROC AUC compared to over-sampling but is still lower than that of under-sampling.\n",
    "\n",
    "2. **Ridge Classifier**:\n",
    "   - The use of class weight adjustment yielded the best results with a ROC AUC of 0.7473, higher than the other resampling methods.\n",
    "   - The performances of the over-sampling and under-sampling methods are relatively similar, with ROC AUCs of 0.7424 and 0.7457, respectively.\n",
    "\n",
    "3. **Custom Cost Metric**:\n",
    "   - The custom cost metric function (`cost_metric(y_true, y_pred)`) has been successfully implemented to create a custom scorer (`make_scorer`) during the GridSearchCV process. This allowed for the minimization of the combined cost of false negatives and false positives, emphasizing the importance of correctly identifying defaulting clients.\n",
    "   - Despite this improvement, there are still areas for further enhancement, particularly in optimizing the balance between precision and recall for defaulting clients.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "1. **For Random Forest**:\n",
    "   - Under-sampling is recommended as it produced the highest ROC AUC. This suggests that reducing the number of majority class samples helps the model learn better.\n",
    "   - Consider tuning the `max_depth` and `n_estimators` further within the under-sampling framework to potentially enhance model performance.\n",
    "\n",
    "2. **For Ridge Classifier**:\n",
    "   - Class weight adjustment is recommended as it achieved the highest ROC AUC, indicating effective handling of class imbalance.\n",
    "   - Further exploration of `alpha` values around 0.1 might yield even better results.\n",
    "\n",
    "3. **Overall Model Performance**:\n",
    "   - The Ridge Classifier with class weight adjustment is the most effective method for this dataset based on the highest ROC AUC.\n",
    "   - Consider implementing and comparing additional models like Gradient Boosting or XGBoost, which can handle imbalanced datasets effectively.\n",
    "\n",
    "4. **Prediction Threshold Optimization**:\n",
    "   - The optimal prediction threshold of 0.2197 was found using the precision-recall curve, which improved overall model accuracy and balanced the precision-recall trade-off.\n",
    "   - Applying this threshold resulted in better handling of the minority class, as evidenced by improved precision and recall metrics in the classification report.\n",
    "\n",
    "### Detailed Evaluation\n",
    "\n",
    "#### Confusion Matrix and Classification Report\n",
    "\n",
    "1. **Without Optimal Threshold**:\n",
    "   - **Confusion Matrix**:\n",
    "     ```\n",
    "     [[39018 17536]\n",
    "      [ 1580  3369]]\n",
    "     ```\n",
    "   - **Classification Report**:\n",
    "     ```\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             0.0       0.96      0.69      0.80     56554\n",
    "             1.0       0.16      0.68      0.26      4949\n",
    "\n",
    "        accuracy                           0.69     61503\n",
    "       macro avg       0.56      0.69      0.53     61503\n",
    "    weighted avg       0.90      0.69      0.76     61503\n",
    "     ```\n",
    "\n",
    "2. **With Optimal Threshold**:\n",
    "   - **Confusion Matrix**:\n",
    "     ```\n",
    "     [[48102  8452]\n",
    "      [ 2602  2347]]\n",
    "     ```\n",
    "   - **Classification Report**:\n",
    "     ```\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "             0.0       0.95      0.85      0.90     56554\n",
    "             1.0       0.22      0.47      0.30      4949\n",
    "\n",
    "        accuracy                           0.82     61503\n",
    "       macro avg       0.58      0.66      0.60     61503\n",
    "    weighted avg       0.89      0.82      0.85     61503\n",
    "     ```\n",
    "\n",
    "#### Cost Evaluation\n",
    "\n",
    "1. **Validation Cost Without Optimal Threshold**: 33336\n",
    "2. **Validation Cost With Optimal Threshold**: 34472\n",
    "\n",
    "### Class-specific Performance\n",
    "\n",
    "- **Class 0 (Non-defaulting clients)**:\n",
    "  - **Precision**: 0.96 (The proportion of clients predicted not to default that actually did not default)\n",
    "  - **Recall**: 0.69 (The proportion of actual non-defaulting clients correctly predicted)\n",
    "  - **F1-Score**: 0.80 (Harmonic mean of precision and recall)\n",
    "  - **Support**: 56,554 (Number of actual non-defaulting clients)\n",
    "\n",
    "- **Class 1 (Defaulting clients)**:\n",
    "  - **Precision**: 0.16 (The proportion of clients predicted to default that actually defaulted)\n",
    "  - **Recall**: 0.68 (The proportion of actual defaulting clients correctly predicted)\n",
    "  - **F1-Score**: 0.26 (Harmonic mean of precision and recall, which is low due to precision being low)\n",
    "  - **Support**: 4,949 (Number of actual defaulting clients)\n",
    "\n",
    "- **Overall Metrics**:\n",
    "  - **Accuracy**: 0.69 (The proportion of total correct predictions)\n",
    "  - **Macro Average**: Average precision, recall, and F1-score for both classes (treating all classes equally)\n",
    "  - **Weighted Average**: Average precision, recall, and F1-score for both classes (considering the support of each class)\n",
    "\n",
    "### Observations and Insights\n",
    "1. **Model Performance**:\n",
    "   - The model shows a significant improvement in recall for defaulting clients (0.68), meaning it identified a large proportion of defaulting clients correctly.\n",
    "   - However, the precision for defaulting clients is quite low (0.16), indicating many false positives.\n",
    "   - The model performs well in predicting non-defaulting clients (Class 0) with high precision (0.96) and reasonable recall (0.69).\n",
    "\n",
    "2. **ROC AUC Score**:\n",
    "   - The ROC AUC score of 0.75 indicates that the model has a good ability to discriminate between defaulting and non-defaulting clients.\n",
    "\n",
    "3. **Imbalance Issue**:\n",
    "   - The model still faces an imbalance issue, as seen in the lower precision for defaulting clients and the disparity between class performances.\n",
    "\n",
    "4. **Need for Improvement**:\n",
    "   - Although the model has improved, there is still room for enhancement, particularly in reducing false positives for defaulting clients.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Random Forest** performs best with under-sampling, suggesting that reducing the majority class samples improves model performance.\n",
    "- **Ridge Classifier** achieves the best results with class weight adjustment, effectively managing class imbalance.\n",
    "- **Prediction Threshold Optimization** improved the handling of the minority class, enhancing precision and recall metrics.\n",
    "- **Custom Cost Metric**: The implementation of the custom cost metric function in GridSearchCV has allowed for more targeted optimization, focusing on minimizing the cost of misclassification. However, further improvements are needed to balance the precision and recall for defaulting clients.\n",
    "- **Overall**: The Ridge Classifier with class weight adjustment and an optimized prediction threshold is recommended for this dataset due to its superior ROC AUC and balanced performance metrics. Further exploration of hyperparameters and additional models could provide incremental improvements."
   ],
   "id": "d7e46fad69b73b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4.Feature Importance",
   "id": "898a240d06f7cede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# SHAP analysis\n",
    "# Explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(best_model_final, X_train)\n",
    "shap_values = explainer(X_val)\n",
    "\n",
    "# 4.1 Global Feature Importance Using SHAP\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "# 4.2 Local Feature Importance Using SHAP\n",
    "# Visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0])"
   ],
   "id": "a0647de49eafa0e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Global Feature Importance Using SHAP",
   "id": "b834e8e4483a1c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Explainer**: Create a SHAP explainer object for the best model's classifier using the training data.\n",
    "2. **Compute SHAP Values**: Generate SHAP values for the test dataset to explain the model's predictions.\n",
    "3. **Global Feature Importance**: Visualize the global feature importance using SHAP summary plot, which provides insights into how each feature contributes to the model's predictions."
   ],
   "id": "d4be0141f61189d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Local Feature Importance Using SHAP",
   "id": "f06f82d571f049c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Visualization**: Set up SHAP's JavaScript visualization framework to enable interactive plots.\n",
    "2. **Generate Local Explanation**: Create a SHAP force plot for a single instance from the test dataset, showing how each feature contributes to the model's prediction for that specific instance.\n",
    "3. **Visualize Prediction Breakdown**: Provide a detailed breakdown of the prediction for the selected instance, illustrating the impact of each feature on the predicted value."
   ],
   "id": "187a45cda5219c68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T16:55:36.659521Z",
     "start_time": "2024-06-21T16:55:02.118116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Assuming the best model is the Ridge Classifier with class weight adjustment\n",
    "best_model_final = RidgeClassifier(alpha=0.1, class_weight='balanced')\n",
    "best_model_final.fit(X_train, y_train)\n",
    "\n",
    "# Explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(best_model_final, X_train)\n",
    "shap_values = explainer(X_val)\n",
    "\n",
    "# 4.1 Global Feature Importance Using SHAP\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "# 4.2 Local Feature Importance Using SHAP\n",
    "# Visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0])\n"
   ],
   "id": "82b25714cfdc9596",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "Ill-conditioned matrix (rcond=1.09721e-18): result may not be accurate.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Explain the model's predictions using SHAP\u001B[39;00m\n\u001B[0;32m      9\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(best_model_final, X_train)\n\u001B[1;32m---> 10\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# 4.1 Global Feature Importance Using SHAP\u001B[39;00m\n\u001B[0;32m     13\u001B[0m shap\u001B[38;5;241m.\u001B[39mplots\u001B[38;5;241m.\u001B[39mbar(shap_values)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\explainers\\_explainer.py:350\u001B[0m, in \u001B[0;36mExplainer.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001B[0m\n\u001B[0;32m    346\u001B[0m         feature_names[j] \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])]\n\u001B[0;32m    349\u001B[0m     \u001B[38;5;66;03m# build an explanation object for this input argument\u001B[39;00m\n\u001B[1;32m--> 350\u001B[0m     out\u001B[38;5;241m.\u001B[39mappend(\u001B[43mExplanation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_values\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpected_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclustering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclustering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhierarchical_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhierarchical_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msliced_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# self.output_names\u001B[39;49;00m\n\u001B[0;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_std\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompute_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtime\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart_time\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# output_shape=output_shape,\u001B[39;49;00m\n\u001B[0;32m    359\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#lower_bounds=v_min, upper_bounds=v_max\u001B[39;49;00m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m out\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\shap\\_explanation.py:152\u001B[0m, in \u001B[0;36mExplanation.__init__\u001B[1;34m(self, values, base_values, data, display_data, instance_names, feature_names, output_names, output_indexes, lower_bounds, upper_bounds, error_std, main_effects, hierarchical_values, clustering, compute_time)\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    150\u001B[0m     base_values \u001B[38;5;241m=\u001B[39m Obj(base_values, [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_dims))\n\u001B[1;32m--> 152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_s \u001B[38;5;241m=\u001B[39m \u001B[43mSlicer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisplay_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdisplay_data\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43minstance_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minstance_names\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mAlias\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstance_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_indexes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput_indexes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_dims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_indexes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlower_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlower_bounds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m    \u001B[49m\u001B[43mupper_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mupper_bounds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_std\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror_std\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhierarchical_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_wrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhierarchical_values\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclustering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclustering\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mObj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclustering\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer.py:55\u001B[0m, in \u001B[0;36mSlicer.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;124;03m\"\"\" Wraps objects in args and provides unified numpy-like slicing.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m    Currently supports (with arbitrary nesting):\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m \n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 55\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m_init_slicer(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer.py:87\u001B[0m, in \u001B[0;36mSlicer._init_slicer\u001B[1;34m(cls, slicer_instance, *args, **kwargs)\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# Go through named objects / aliases\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 87\u001B[0m     \u001B[43mslicer_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__setattr__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# Generate default aliases only if one object and no aliases exist\u001B[39;00m\n\u001B[0;32m     90\u001B[0m objects_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(slicer_instance\u001B[38;5;241m.\u001B[39m_objects)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer.py:198\u001B[0m, in \u001B[0;36mSlicer.__setattr__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_alias_lookup\u001B[38;5;241m.\u001B[39mdelete(old_alias) \n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_aliases[key]\n\u001B[1;32m--> 198\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[43mObj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, Obj) \u001B[38;5;28;01melse\u001B[39;00m value\n\u001B[0;32m    199\u001B[0m value\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m key\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_objects[key] \u001B[38;5;241m=\u001B[39m value\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:242\u001B[0m, in \u001B[0;36mObj.__init__\u001B[1;34m(self, o, dim)\u001B[0m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, o, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 242\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:219\u001B[0m, in \u001B[0;36mTracked.__init__\u001B[1;34m(self, o, dim)\u001B[0m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, o: Any, dim: Union[\u001B[38;5;28mint\u001B[39m, List, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;124;03m\"\"\" Defines an object that will be wrapped by slicer.\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \n\u001B[0;32m    215\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;124;03m        o: Object that will be tracked for slicer.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;124;03m        dim: Target dimension(s) slicer will index on for this object.\u001B[39;00m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 219\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# Protected attribute that can be overriden.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:43\u001B[0m, in \u001B[0;36mAtomicSlicer.__init__\u001B[1;34m(self, o, max_dim)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_dim \u001B[38;5;241m=\u001B[39m max_dim\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_dim \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 43\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_dim \u001B[38;5;241m=\u001B[39m \u001B[43mUnifiedDataHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:591\u001B[0m, in \u001B[0;36mUnifiedDataHandler.max_dim\u001B[1;34m(cls, o)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m o_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mtype_map:\n\u001B[0;32m    590\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mo_type\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:473\u001B[0m, in \u001B[0;36mArrayHandler.max_dim\u001B[1;34m(cls, o)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_dim\u001B[39m(\u001B[38;5;28mcls\u001B[39m, o):\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _safe_isinstance(o, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m o\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 473\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m([UnifiedDataHandler\u001B[38;5;241m.\u001B[39mmax_dim(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m o], default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    475\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(o\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:473\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_dim\u001B[39m(\u001B[38;5;28mcls\u001B[39m, o):\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _safe_isinstance(o, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m o\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 473\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m([\u001B[43mUnifiedDataHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m o], default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    475\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(o\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:591\u001B[0m, in \u001B[0;36mUnifiedDataHandler.max_dim\u001B[1;34m(cls, o)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m o_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mtype_map:\n\u001B[0;32m    590\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mo_type\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:473\u001B[0m, in \u001B[0;36mArrayHandler.max_dim\u001B[1;34m(cls, o)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_dim\u001B[39m(\u001B[38;5;28mcls\u001B[39m, o):\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _safe_isinstance(o, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m o\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 473\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m([UnifiedDataHandler\u001B[38;5;241m.\u001B[39mmax_dim(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m o], default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    475\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(o\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:473\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_dim\u001B[39m(\u001B[38;5;28mcls\u001B[39m, o):\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _safe_isinstance(o, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m o\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 473\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m([\u001B[43mUnifiedDataHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m o], default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    475\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(o\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:591\u001B[0m, in \u001B[0;36mUnifiedDataHandler.max_dim\u001B[1;34m(cls, o)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m o_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mtype_map:\n\u001B[0;32m    590\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mo_type\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:472\u001B[0m, in \u001B[0;36mArrayHandler.max_dim\u001B[1;34m(cls, o)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_dim\u001B[39m(\u001B[38;5;28mcls\u001B[39m, o):\n\u001B[1;32m--> 472\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m_safe_isinstance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnumpy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mndarray\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m o\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    473\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m([UnifiedDataHandler\u001B[38;5;241m.\u001B[39mmax_dim(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m o], default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\slicer\\slicer_internal.py:609\u001B[0m, in \u001B[0;36m_safe_isinstance\u001B[1;34m(o, module_name, type_name)\u001B[0m\n\u001B[0;32m    605\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_safe_isinstance\u001B[39m(\n\u001B[0;32m    606\u001B[0m     o: \u001B[38;5;28mobject\u001B[39m, module_name: \u001B[38;5;28mstr\u001B[39m, type_name: Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mset\u001B[39m, \u001B[38;5;28mtuple\u001B[39m]\n\u001B[0;32m    607\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m    608\u001B[0m     o_module, o_type \u001B[38;5;241m=\u001B[39m _type_name(o)\n\u001B[1;32m--> 609\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m(type_name, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    610\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m o_module \u001B[38;5;241m==\u001B[39m module_name \u001B[38;5;129;01mand\u001B[39;00m o_type \u001B[38;5;241m==\u001B[39m type_name\n\u001B[0;32m    611\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Glossary\n",
    "\n",
    "### Cross-Validation\n",
    "A technique used to assess the performance of a model by splitting the dataset into multiple training and testing sets. This helps in understanding how the model will generalize to an independent dataset.\n",
    "\n",
    "### GridSearchCV\n",
    "A tool from `scikit-learn` that performs hyperparameter tuning by exhaustively searching through a specified parameter grid to find the best combination of hyperparameters for a given model, using cross-validation.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "The process of finding the optimal values for hyperparameters of a model, which are parameters that are not learned from data but set before the learning process begins. Examples include the number of trees in a random forest or the regularization strength in Ridge regression.\n",
    "\n",
    "### ROC AUC Score\n",
    "A performance measurement for classification problems at various threshold settings. ROC AUC represents the area under the Receiver Operating Characteristic curve, which plots the true positive rate against the false positive rate. A score closer to 1 indicates better performance.\n",
    "\n",
    "### Confusion Matrix\n",
    "A table used to describe the performance of a classification model by showing the actual vs. predicted classifications. It includes True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "### Classification Report\n",
    "A detailed report showing the precision, recall, F1-score, and support for each class in a classification problem. This helps in understanding the performance of the model across different classes.\n",
    "\n",
    "### Resampling Techniques\n",
    "Methods used to adjust the class distribution of a dataset, commonly used to handle class imbalance. Examples include:\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic samples for the minority class.\n",
    "- **RandomUnderSampler**: Reduces the number of samples in the majority class.\n",
    "\n",
    "### Pipeline\n",
    "A tool from `scikit-learn` that allows for chaining multiple processing steps (e.g., data transformation and model fitting) into a single object. This ensures that all steps are applied consistently during both training and testing.\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations)\n",
    "A method to explain individual predictions of machine learning models by assigning each feature an importance value. It helps in understanding how the model arrives at its predictions.\n",
    "\n",
    "### SHAP Explainer\n",
    "An object in the SHAP library that is used to calculate SHAP values for a given model. It helps in interpreting the contributions of each feature to the predictions.\n",
    "\n",
    "### SHAP Values\n",
    "Values calculated by the SHAP explainer that quantify the contribution of each feature to the model's prediction for a given instance. Higher absolute values indicate greater impact on the prediction.\n",
    "\n",
    "### SHAP Summary Plot\n",
    "A plot that visualizes the global importance of features by showing the distribution of SHAP values for each feature across all instances in the dataset. It helps in understanding which features are most influential for the model.\n",
    "\n",
    "### SHAP Force Plot\n",
    "A plot that provides a detailed breakdown of the contributions of each feature to a single instance's prediction, illustrating how different features push the prediction towards or away from the base value.\n",
    "\n",
    "### Random Forest\n",
    "An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "### Ridge Classifier\n",
    "A linear model for classification that includes L2 regularization (Ridge regression) to prevent overfitting by penalizing large coefficients.\n",
    "\n",
    "### Class Weight\n",
    "A parameter used to handle class imbalance by assigning different weights to different classes, typically to give more importance to the minority class. This can be set to 'balanced' to automatically adjust weights inversely proportional to class frequencies.\n",
    "\n",
    "### RandomUnderSampler\n",
    "A resampling technique that reduces the number of instances in the majority class by randomly sampling without replacement, used to balance the class distribution.\n",
    "\n",
    "### SMOTE\n",
    "A resampling technique that generates synthetic samples for the minority class by interpolating between existing minority class instances. It is used to balance the class distribution in the dataset.\n"
   ],
   "id": "27de101d1c04a5e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
