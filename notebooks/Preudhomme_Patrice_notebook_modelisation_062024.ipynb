{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the Preprocessed Data",
   "id": "458083ce2249405a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import gc\n",
    "\n",
    "# Indicate if the script is running on Google Colab or not\n",
    "using_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if using_colab:\n",
    "    # Connect Google Drive to Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    \n",
    "    # Install necessary packages\n",
    "    !pip install numpy\n",
    "    !pip install pandas\n",
    "    !pip install sklearn\n",
    "    !pip install matplotlib\n",
    "    !pip install seaborn\n",
    "    !pip install imbalanced-learn\n",
    "    !pip install shap\n",
    "    \n",
    "    # Path for Google Colab\n",
    "    project_root = '/content/gdrive/MyDrive/oc_projet_4/' \n",
    "else:\n",
    "    # Get the current working directory as base directory for the notebook\n",
    "    base_dir = os.getcwd()\n",
    "    \n",
    "    # Adjust the project root path relatively to where the notebook is located\n",
    "    # Assuming the notebook is inside a 'notebooks' directory and we need to go up one level to access project root\n",
    "    project_root = os.path.join(base_dir, '..')\n",
    "\n",
    "# Clean output of cell\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "# Load the CSV files\n",
    "train_domain_path = os.path.join(project_root, 'data', 'app_train_domain.csv')\n",
    "test_domain_path = os.path.join(project_root, 'data', 'app_test_domain.csv')\n",
    "\n",
    "app_train_domain = pd.read_csv(train_domain_path)\n",
    "app_test_domain = pd.read_csv(test_domain_path)\n",
    "\n",
    "# Extract the training and testing sets\n",
    "X_train = app_train_domain.drop(columns=['TARGET']).values\n",
    "y_train = app_train_domain['TARGET'].values\n",
    "X_test = app_test_domain.values  # Assuming test set does not have TARGET column\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Data loaded successfully.')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_val shape: {X_val_split.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_val shape: {y_val_split.shape}')\n"
   ],
   "id": "bd48a92e09961fda",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "X_train shape: (307511, 247)\n",
      "X_val shape: (61503, 247)\n",
      "X_test shape: (48744, 248)\n",
      "y_train shape: (307511,)\n",
      "y_val shape: (61503,)\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T19:26:02.924421Z",
     "start_time": "2024-06-16T19:26:02.895425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the first few values of y_train\n",
    "print('First 10 values of y_train:', y_train[:10])\n",
    "\n",
    "# Check the unique values in y_train\n",
    "print('Unique values in y_train:', np.unique(y_train))\n",
    "\n",
    "# Check for missing values in y_train\n",
    "print('Number of missing values in y_train:', pd.Series(y_train).isnull().sum())"
   ],
   "id": "e3ec5c9abc0dbc05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values of y_train: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Unique values in y_train: [0. 1.]\n",
      "Number of missing values in y_train: 0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T19:26:03.096456Z",
     "start_time": "2024-06-16T19:26:02.928411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the first few rows of the training data\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_train sample values: {y_train[:5]}\")  # Afficher les premières valeurs pour vérification\n",
    "\n",
    "# Display the columns of X_train to verify the features\n",
    "x_train_columns = list(app_train_domain.drop(columns=['TARGET']).columns)\n",
    "print(f\"X_train columns: {x_train_columns}\")\n",
    "\n",
    "# List of new features to check\n",
    "new_features = ['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']\n",
    "\n",
    "# Check if these new features are in X_train columns\n",
    "missing_features = [feature for feature in new_features if feature not in x_train_columns]\n",
    "\n",
    "if not missing_features:\n",
    "    print(\"All new features are present in X_train.\")\n",
    "else:\n",
    "    print(f\"Missing features in X_train: {missing_features}\")\n"
   ],
   "id": "27c15317a12bdd7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (307511,)\n",
      "y_train sample values: [1. 0. 0. 0. 0.]\n",
      "X_train columns: ['SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'CODE_GENDER_F', 'CODE_GENDER_M', 'CODE_GENDER_XNA', 'NAME_TYPE_SUITE_Children', 'NAME_TYPE_SUITE_Family', 'NAME_TYPE_SUITE_Group of people', 'NAME_TYPE_SUITE_Other_A', 'NAME_TYPE_SUITE_Other_B', 'NAME_TYPE_SUITE_Spouse, partner', 'NAME_TYPE_SUITE_Unaccompanied', 'NAME_INCOME_TYPE_Businessman', 'NAME_INCOME_TYPE_Commercial associate', 'NAME_INCOME_TYPE_Maternity leave', 'NAME_INCOME_TYPE_Pensioner', 'NAME_INCOME_TYPE_State servant', 'NAME_INCOME_TYPE_Student', 'NAME_INCOME_TYPE_Unemployed', 'NAME_INCOME_TYPE_Working', 'NAME_EDUCATION_TYPE_Academic degree', 'NAME_EDUCATION_TYPE_Higher education', 'NAME_EDUCATION_TYPE_Incomplete higher', 'NAME_EDUCATION_TYPE_Lower secondary', 'NAME_EDUCATION_TYPE_Secondary / secondary special', 'NAME_FAMILY_STATUS_Civil marriage', 'NAME_FAMILY_STATUS_Married', 'NAME_FAMILY_STATUS_Separated', 'NAME_FAMILY_STATUS_Single / not married', 'NAME_FAMILY_STATUS_Unknown', 'NAME_FAMILY_STATUS_Widow', 'NAME_HOUSING_TYPE_Co-op apartment', 'NAME_HOUSING_TYPE_House / apartment', 'NAME_HOUSING_TYPE_Municipal apartment', 'NAME_HOUSING_TYPE_Office apartment', 'NAME_HOUSING_TYPE_Rented apartment', 'NAME_HOUSING_TYPE_With parents', 'OCCUPATION_TYPE_Accountants', 'OCCUPATION_TYPE_Cleaning staff', 'OCCUPATION_TYPE_Cooking staff', 'OCCUPATION_TYPE_Core staff', 'OCCUPATION_TYPE_Drivers', 'OCCUPATION_TYPE_HR staff', 'OCCUPATION_TYPE_High skill tech staff', 'OCCUPATION_TYPE_IT staff', 'OCCUPATION_TYPE_Laborers', 'OCCUPATION_TYPE_Low-skill Laborers', 'OCCUPATION_TYPE_Managers', 'OCCUPATION_TYPE_Medicine staff', 'OCCUPATION_TYPE_Private service staff', 'OCCUPATION_TYPE_Realty agents', 'OCCUPATION_TYPE_Sales staff', 'OCCUPATION_TYPE_Secretaries', 'OCCUPATION_TYPE_Security staff', 'OCCUPATION_TYPE_Waiters/barmen staff', 'WEEKDAY_APPR_PROCESS_START_FRIDAY', 'WEEKDAY_APPR_PROCESS_START_MONDAY', 'WEEKDAY_APPR_PROCESS_START_SATURDAY', 'WEEKDAY_APPR_PROCESS_START_SUNDAY', 'WEEKDAY_APPR_PROCESS_START_THURSDAY', 'WEEKDAY_APPR_PROCESS_START_TUESDAY', 'WEEKDAY_APPR_PROCESS_START_WEDNESDAY', 'ORGANIZATION_TYPE_Advertising', 'ORGANIZATION_TYPE_Agriculture', 'ORGANIZATION_TYPE_Bank', 'ORGANIZATION_TYPE_Business Entity Type 1', 'ORGANIZATION_TYPE_Business Entity Type 2', 'ORGANIZATION_TYPE_Business Entity Type 3', 'ORGANIZATION_TYPE_Cleaning', 'ORGANIZATION_TYPE_Construction', 'ORGANIZATION_TYPE_Culture', 'ORGANIZATION_TYPE_Electricity', 'ORGANIZATION_TYPE_Emergency', 'ORGANIZATION_TYPE_Government', 'ORGANIZATION_TYPE_Hotel', 'ORGANIZATION_TYPE_Housing', 'ORGANIZATION_TYPE_Industry: type 1', 'ORGANIZATION_TYPE_Industry: type 10', 'ORGANIZATION_TYPE_Industry: type 11', 'ORGANIZATION_TYPE_Industry: type 12', 'ORGANIZATION_TYPE_Industry: type 13', 'ORGANIZATION_TYPE_Industry: type 2', 'ORGANIZATION_TYPE_Industry: type 3', 'ORGANIZATION_TYPE_Industry: type 4', 'ORGANIZATION_TYPE_Industry: type 5', 'ORGANIZATION_TYPE_Industry: type 6', 'ORGANIZATION_TYPE_Industry: type 7', 'ORGANIZATION_TYPE_Industry: type 8', 'ORGANIZATION_TYPE_Industry: type 9', 'ORGANIZATION_TYPE_Insurance', 'ORGANIZATION_TYPE_Kindergarten', 'ORGANIZATION_TYPE_Legal Services', 'ORGANIZATION_TYPE_Medicine', 'ORGANIZATION_TYPE_Military', 'ORGANIZATION_TYPE_Mobile', 'ORGANIZATION_TYPE_Other', 'ORGANIZATION_TYPE_Police', 'ORGANIZATION_TYPE_Postal', 'ORGANIZATION_TYPE_Realtor', 'ORGANIZATION_TYPE_Religion', 'ORGANIZATION_TYPE_Restaurant', 'ORGANIZATION_TYPE_School', 'ORGANIZATION_TYPE_Security', 'ORGANIZATION_TYPE_Security Ministries', 'ORGANIZATION_TYPE_Self-employed', 'ORGANIZATION_TYPE_Services', 'ORGANIZATION_TYPE_Telecom', 'ORGANIZATION_TYPE_Trade: type 1', 'ORGANIZATION_TYPE_Trade: type 2', 'ORGANIZATION_TYPE_Trade: type 3', 'ORGANIZATION_TYPE_Trade: type 4', 'ORGANIZATION_TYPE_Trade: type 5', 'ORGANIZATION_TYPE_Trade: type 6', 'ORGANIZATION_TYPE_Trade: type 7', 'ORGANIZATION_TYPE_Transport: type 1', 'ORGANIZATION_TYPE_Transport: type 2', 'ORGANIZATION_TYPE_Transport: type 3', 'ORGANIZATION_TYPE_Transport: type 4', 'ORGANIZATION_TYPE_University', 'ORGANIZATION_TYPE_XNA', 'FONDKAPREMONT_MODE_not specified', 'FONDKAPREMONT_MODE_org spec account', 'FONDKAPREMONT_MODE_reg oper account', 'FONDKAPREMONT_MODE_reg oper spec account', 'HOUSETYPE_MODE_block of flats', 'HOUSETYPE_MODE_specific housing', 'HOUSETYPE_MODE_terraced house', 'WALLSMATERIAL_MODE_Block', 'WALLSMATERIAL_MODE_Mixed', 'WALLSMATERIAL_MODE_Monolithic', 'WALLSMATERIAL_MODE_Others', 'WALLSMATERIAL_MODE_Panel', 'WALLSMATERIAL_MODE_Stone, brick', 'WALLSMATERIAL_MODE_Wooden', 'EMERGENCYSTATE_MODE_No', 'EMERGENCYSTATE_MODE_Yes', 'DAYS_EMPLOYED_ANOM', 'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']\n",
      "All new features are present in X_train.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T19:26:03.239979Z",
     "start_time": "2024-06-16T19:26:03.098453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verify the data types and missing values in the training data\n",
    "print(app_train_domain.dtypes)\n",
    "print(app_train_domain.isnull().sum())\n"
   ],
   "id": "4bdac84c3299f277",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK_ID_CURR                  int64\n",
      "TARGET                    float64\n",
      "NAME_CONTRACT_TYPE          int64\n",
      "FLAG_OWN_CAR                int64\n",
      "FLAG_OWN_REALTY             int64\n",
      "                           ...   \n",
      "DAYS_EMPLOYED_ANOM           bool\n",
      "CREDIT_INCOME_PERCENT     float64\n",
      "ANNUITY_INCOME_PERCENT    float64\n",
      "CREDIT_TERM               float64\n",
      "DAYS_EMPLOYED_PERCENT     float64\n",
      "Length: 248, dtype: object\n",
      "SK_ID_CURR                0\n",
      "TARGET                    0\n",
      "NAME_CONTRACT_TYPE        0\n",
      "FLAG_OWN_CAR              0\n",
      "FLAG_OWN_REALTY           0\n",
      "                         ..\n",
      "DAYS_EMPLOYED_ANOM        0\n",
      "CREDIT_INCOME_PERCENT     0\n",
      "ANNUITY_INCOME_PERCENT    0\n",
      "CREDIT_TERM               0\n",
      "DAYS_EMPLOYED_PERCENT     0\n",
      "Length: 248, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.Hyperparameter Tuning via GridSearchCV and Imbalanced-learn Pipeline",
   "id": "cbe789d731d07bcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Set Up Resampling Techniques**: Define over-sampling (SMOTE) and under-sampling (RandomUnderSampler) methods to handle class imbalance in the dataset.\n",
    "2. **Initialize Models**: Create instances of RandomForestClassifier and RidgeClassifier for classification tasks.\n",
    "3. **Define Hyperparameter Grids**: Specify parameter grids for tuning hyperparameters of the classifiers using GridSearchCV.\n",
    "4. **Create Pipelines**: Construct pipelines to integrate resampling techniques with the classifiers.\n",
    "5. **Perform Hyperparameter Tuning**: Use GridSearchCV to find the best hyperparameters and resampling techniques for each classifier, evaluating them using cross-validation and storing the best models."
   ],
   "id": "6ea95384359ab660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:03:00.767691Z",
     "start_time": "2024-06-16T19:26:03.242979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the resampling techniques\n",
    "over_sampler = SMOTE(random_state=42)\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Define the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "ridge = RidgeClassifier()\n",
    "\n",
    "# Define the parameter grids for GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [10, 20]\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'classifier__alpha': [1.0, 0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Define pipelines with normalization\n",
    "pipeline_rf_over = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('oversample', over_sampler),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "pipeline_rf_under = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('undersample', under_sampler),\n",
    "    ('classifier', rf)\n",
    "])\n",
    "\n",
    "pipeline_rf_weight = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_ridge_over = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('oversample', over_sampler),\n",
    "    ('classifier', ridge)\n",
    "])\n",
    "\n",
    "pipeline_ridge_under = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('undersample', under_sampler),\n",
    "    ('classifier', ridge)\n",
    "])\n",
    "\n",
    "pipeline_ridge_weight = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RidgeClassifier(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Perform GridSearchCV for each pipeline and store the best estimators\n",
    "grids = [\n",
    "    (pipeline_rf_over, param_grid_rf, 'Random Forest with Over-sampling'),\n",
    "    (pipeline_rf_under, param_grid_rf, 'Random Forest with Under-sampling'),\n",
    "    (pipeline_rf_weight, param_grid_rf, 'Random Forest with Class Weight'),\n",
    "    (pipeline_ridge_over, param_grid_ridge, 'Ridge with Over-sampling'),\n",
    "    (pipeline_ridge_under, param_grid_ridge, 'Ridge with Under-sampling'),\n",
    "    (pipeline_ridge_weight, param_grid_ridge, 'Ridge with Class Weight')\n",
    "]\n",
    "\n",
    "best_estimators = {}\n",
    "\n",
    "for pipeline, param_grid, name in grids:\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='roc_auc', cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train_split, y_train_split)\n",
    "    best_estimators[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best ROC AUC for {name}: {grid_search.best_score_}\")\n",
    "\n",
    "gc.collect()\n"
   ],
   "id": "e114618477f0c6d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest with Over-sampling: {'classifier__max_depth': 20, 'classifier__n_estimators': 200}\n",
      "Best ROC AUC for Random Forest with Over-sampling: 0.7007867231242678\n",
      "Best parameters for Random Forest with Under-sampling: {'classifier__max_depth': 20, 'classifier__n_estimators': 200}\n",
      "Best ROC AUC for Random Forest with Under-sampling: 0.7393563922932804\n",
      "Best parameters for Random Forest with Class Weight: {'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n",
      "Best ROC AUC for Random Forest with Class Weight: 0.728529571984371\n",
      "Best parameters for Ridge with Over-sampling: {'classifier__alpha': 1.0}\n",
      "Best ROC AUC for Ridge with Over-sampling: 0.7367378045291945\n",
      "Best parameters for Ridge with Under-sampling: {'classifier__alpha': 1.0}\n",
      "Best ROC AUC for Ridge with Under-sampling: 0.7445430380772825\n",
      "Best parameters for Ridge with Class Weight: {'classifier__alpha': 1.0}\n",
      "Best ROC AUC for Ridge with Class Weight: 0.7467052825407473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.Evaluate the Best Model",
   "id": "97552594f71e4aeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Select the Best Model**: Identify and select the best model from the hyperparameter-tuned models stored in `best_estimators` based on their performance on the test set.\n",
    "2. **Make Predictions**: Use the selected best model to predict the target variable on the test dataset.\n",
    "3. **Evaluate Performance**: Calculate the confusion matrix and ROC AUC score to assess the model's performance on the test data.\n",
    "4. **Check for Overfitting**: Ensure that the ROC AUC score is below 0.82 to avoid overfitting, issuing a warning if the score is higher.\n",
    "5. **Generate Detailed Metrics**: Print a comprehensive classification report including precision, recall, and F1-score for a detailed evaluation of the model's performance."
   ],
   "id": "3208e4b1a5c980ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:03:18.910592Z",
     "start_time": "2024-06-16T20:03:00.770690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "# Select the best model (for example purposes, choosing the best model based on GridSearchCV results)\n",
    "best_model_name = max(best_estimators, key=lambda name: best_estimators[name].score(X_val_split, y_val_split))\n",
    "best_model = best_estimators[best_model_name]\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_model.predict(X_val_split)\n",
    "\n",
    "# Compute confusion matrix for validation set\n",
    "conf_matrix_val = confusion_matrix(y_val_split, y_val_pred)\n",
    "print(\"Confusion Matrix (Validation):\")\n",
    "print(conf_matrix_val)\n",
    "\n",
    "# Compute ROC AUC score for validation set\n",
    "roc_auc_val = roc_auc_score(y_val_split, y_val_pred)\n",
    "print(f\"ROC AUC (Validation): {roc_auc_val}\")\n",
    "\n",
    "# Ensure ROC AUC < 0.82 for validation set\n",
    "if roc_auc_val >= 0.82:\n",
    "    print(\"Warning: ROC AUC score (Validation) is greater than or equal to 0.82. Model might be overfitting.\")\n",
    "\n",
    "# Print classification report for validation set\n",
    "print(\"Classification Report (Validation):\")\n",
    "print(classification_report(y_val_split, y_val_pred))\n"
   ],
   "id": "2033f56a3dd17be4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Validation):\n",
      "[[55304  1250]\n",
      " [ 4533   416]]\n",
      "ROC AUC (Validation): 0.5309773081477326\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.98      0.95     56554\n",
      "         1.0       0.25      0.08      0.13      4949\n",
      "\n",
      "    accuracy                           0.91     61503\n",
      "   macro avg       0.59      0.53      0.54     61503\n",
      "weighted avg       0.87      0.91      0.88     61503\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results!!!!\n",
    "\n",
    "#### Summary\n",
    "\n",
    "1. **Random Forest with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__class_weight': None, 'classifier__max_depth': 20, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.706770540515098`\n",
    "\n",
    "2. **Random Forest with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__class_weight': None, 'classifier__max_depth': 20, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.739534466894729`\n",
    "\n",
    "3. **Random Forest with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__class_weight': None, 'classifier__max_depth': 10, 'classifier__n_estimators': 200}`\n",
    "   - **Best ROC AUC**: `0.7340727428197145`\n",
    "\n",
    "4. **Ridge with Over-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0, 'classifier__class_weight': None}`\n",
    "   - **Best ROC AUC**: `0.745725332792906`\n",
    "\n",
    "5. **Ridge with Under-sampling**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0, 'classifier__class_weight': None}`\n",
    "   - **Best ROC AUC**: `0.7446270382358854`\n",
    "\n",
    "6. **Ridge with Class Weight**:\n",
    "   - **Best Parameters**: `{'classifier__alpha': 1.0, 'classifier__class_weight': 'balanced'}`\n",
    "   - **Best ROC AUC**: `0.746318807175601`\n",
    "\n",
    "#### Observations\n",
    "\n",
    "1. **Random Forest**:\n",
    "   - The best parameters for both over-sampling and under-sampling are the same, suggesting that the model configuration is robust across different resampling strategies.\n",
    "   - The ROC AUC for under-sampling (0.7395) is higher than for over-sampling (0.7068), indicating that under-sampling might be a better strategy for the Random Forest model in this case.\n",
    "   - Using class weight adjustment in Random Forest shows an improvement in ROC AUC compared to over-sampling but still underperforms compared to under-sampling.\n",
    "\n",
    "2. **Ridge Classifier**:\n",
    "   - The best parameters for both over-sampling and under-sampling are the same.\n",
    "   - The ROC AUC values are very close for over-sampling (0.7457) and under-sampling (0.7446), indicating that both resampling strategies perform similarly well for the Ridge Classifier.\n",
    "   - Using class weight adjustment in Ridge Classifier gives a slightly better ROC AUC (0.7463), making it the best-performing method overall.\n",
    "\n",
    "#### Warnings\n",
    "\n",
    "The warnings you received:\n",
    "```\n",
    "C:\\Users\\pat\\.conda\\envs\\P4\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=7.51842e-18): result may not be accurate.\n",
    "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
    "\n",
    "C:\\Users\\pat\\.conda\\envs\\P4\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=6.55936e-17): result may not be accurate.\n",
    "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
    "```\n",
    "These warnings indicate that the Ridge regression encountered ill-conditioned matrices, which means that the matrix used in the ridge regression solver has very small singular values, causing potential numerical instability. This can happen when there are highly correlated features in the dataset.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "1. **Addressing Ill-Conditioned Matrix Warnings**:\n",
    "   - **Feature Engineering**: Investigate and possibly remove or combine highly correlated features to reduce multicollinearity.\n",
    "   - **Regularization**: Consider using stronger regularization (increase the `alpha` parameter) to help stabilize the solution.\n",
    "   - **Scaling**: Ensure that the features are properly scaled, as Ridge regression can be sensitive to the scale of the features.\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - **Ridge Classifier**: Given the higher ROC AUC scores, the Ridge Classifier seems to perform better than the Random Forest in this case.\n",
    "   - **Resampling Strategy**: Both over-sampling and under-sampling show similar performance for the Ridge Classifier, but under-sampling performs better for Random Forest. You might choose the strategy based on other factors such as computational efficiency or interpretability."
   ],
   "id": "56de0e2246ebb5d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3.!!!!!!!!\n",
    "1. **Model Evaluation**:\n",
    "   - Evaluate the best models on the test set using confusion matrix, ROC AUC, and other metrics.\n",
    "\n",
    "2. **Feature Importance**:\n",
    "   - Use SHAP or similar methods to interpret the models and understand the importance of different features.\n",
    "\n",
    "3. **Documentation and Reporting**:\n",
    "   - Document the findings, including the best parameters, ROC AUC scores, and any observations regarding the resampling strategies and model performance."
   ],
   "id": "1243d29a0c120335"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of the Results\n",
    "\n",
    "#### Confusion Matrix\n",
    "```\n",
    "Confusion Matrix:\n",
    "[[84806     0]\n",
    " [ 7447     1]]\n",
    "```\n",
    "The confusion matrix shows:\n",
    "- True Negatives (TN): 84,806 (clients correctly identified as not defaulting)\n",
    "- False Positives (FP): 0 (clients incorrectly identified as defaulting)\n",
    "- False Negatives (FN): 7,447 (clients incorrectly identified as not defaulting)\n",
    "- True Positives (TP): 1 (clients correctly identified as defaulting)\n",
    "\n",
    "#### ROC AUC Score\n",
    "```\n",
    "ROC AUC: 0.5000671321160043\n",
    "```\n",
    "The ROC AUC score is approximately 0.50, which is equivalent to random guessing. This indicates that the model has no discriminative power in distinguishing between classes (default vs. non-default).\n",
    "\n",
    "#### Classification Report\n",
    "```\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      1.00      0.96     84806\n",
    "           1       1.00      0.00      0.00      7448\n",
    "\n",
    "    accuracy                           0.92     92254\n",
    "   macro avg       0.96      0.50      0.48     92254\n",
    "weighted avg       0.93      0.92      0.88     92254\n",
    "```\n",
    "- **Class 0 (Non-defaulting clients)**:\n",
    "  - Precision: 0.92 (The proportion of clients predicted not to default that actually did not default)\n",
    "  - Recall: 1.00 (The proportion of actual non-defaulting clients correctly predicted)\n",
    "  - F1-Score: 0.96 (Harmonic mean of precision and recall)\n",
    "  - Support: 84,806 (Number of actual non-defaulting clients)\n",
    "\n",
    "- **Class 1 (Defaulting clients)**:\n",
    "  - Precision: 1.00 (The proportion of clients predicted to default that actually defaulted)\n",
    "  - Recall: 0.00 (The proportion of actual defaulting clients correctly predicted)\n",
    "  - F1-Score: 0.00 (Harmonic mean of precision and recall, which is 0 due to recall being 0)\n",
    "  - Support: 7,448 (Number of actual defaulting clients)\n",
    "\n",
    "- **Overall Metrics**:\n",
    "  - Accuracy: 0.92 (The proportion of total correct predictions)\n",
    "  - Macro Average: Average precision, recall, and F1-score for both classes (treating all classes equally)\n",
    "  - Weighted Average: Average precision, recall, and F1-score for both classes (considering the support of each class)\n",
    "\n",
    "### Observations and Insights\n",
    "1. **Model Performance**:\n",
    "   - The model performs exceptionally well in predicting non-defaulting clients (Class 0) but fails to predict defaulting clients (Class 1).\n",
    "   - The recall for defaulting clients is 0, indicating that the model did not identify any defaulting clients correctly.\n",
    "\n",
    "2. **ROC AUC Score**:\n",
    "   - The ROC AUC score of 0.50 indicates that the model has no discriminative ability, performing no better than random guessing.\n",
    "\n",
    "3. **Imbalance Issue**:\n",
    "   - The classification report and confusion matrix highlight a significant class imbalance issue, where the model is biased towards the majority class (non-defaulting clients).\n",
    "\n",
    "4. **Need for Improvement**:\n",
    "   - The current model is inadequate for practical use in credit scoring due to its failure to identify defaulting clients.\n",
    "   - Resampling techniques (like SMOTE, under-sampling, and class weighting) or different models need to be explored further to address the imbalance and improve the model’s performance.\n",
    "\n",
    "### Recommendations\n",
    "1. **Reassess the Model**:\n",
    "   - Consider using more sophisticated techniques for handling class imbalance, such as SMOTE with Tomek links or ensemble methods like balanced random forests.\n",
    "   - Experiment with other classifiers that might handle imbalance better, such as XGBoost or LightGBM.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Review and improve feature engineering to ensure relevant features are being used, which might help the model differentiate better between classes.\n",
    "\n",
    "3. **Evaluate Data**:\n",
    "   - Ensure data quality and consider additional preprocessing steps to handle any underlying issues that might be affecting model performance.\n",
    "\n",
    "By focusing on improving the handling of class imbalance and re-evaluating the features used, the model's ability to predict both defaulting and non-defaulting clients can be significantly enhanced."
   ],
   "id": "4bb0960554d4770"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4.Feature Importance",
   "id": "898a240d06f7cede"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 Global Feature Importance Using SHAP",
   "id": "b834e8e4483a1c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Explainer**: Create a SHAP explainer object for the best model's classifier using the training data.\n",
    "2. **Compute SHAP Values**: Generate SHAP values for the test dataset to explain the model's predictions.\n",
    "3. **Global Feature Importance**: Visualize the global feature importance using SHAP summary plot, which provides insights into how each feature contributes to the model's predictions."
   ],
   "id": "d4be0141f61189d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T20:03:27.640573Z",
     "start_time": "2024-06-16T20:03:18.912598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the data to a DataFrame if it's not already one\n",
    "X_train_split_df = pd.DataFrame(X_train_split)\n",
    "X_val_split_df = pd.DataFrame(X_val_split)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "# Check and convert categorical variables to numeric\n",
    "X_train_split_df = pd.get_dummies(X_train_split_df)\n",
    "X_val_split_df = pd.get_dummies(X_val_split_df)\n",
    "X_test_df = pd.get_dummies(X_test_df)\n",
    "\n",
    "# Align the training, validation, and testing data to have the same columns\n",
    "X_train_split_df, X_val_split_df = X_train_split_df.align(X_val_split_df, join='inner', axis=1)\n",
    "X_train_split_df, X_test_df = X_train_split_df.align(X_test_df, join='inner', axis=1)\n",
    "X_val_split_df, X_test_df = X_val_split_df.align(X_test_df, join='inner', axis=1)\n",
    "\n",
    "# Fit the explainer on the training data\n",
    "explainer = shap.Explainer(best_model.named_steps['classifier'], X_train_split_df)\n",
    "shap_values = explainer(X_val_split_df)\n",
    "\n",
    "# Global feature importance\n",
    "shap.summary_plot(shap_values, X_val_split_df)\n",
    "\n",
    "# Local explanation for a single instance\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_val_split_df.iloc[0,:])\n",
    "\n"
   ],
   "id": "ecd18ea859ae31f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 56.4 GiB for an array with shape (246008, 246008) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m X_test_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(X_test)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Check and convert categorical variables to numeric\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m X_train_split_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dummies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_split_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m X_val_split_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(X_val_split_df)\n\u001B[0;32m     12\u001B[0m X_test_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(X_test_df)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\pandas\\core\\reshape\\encoding.py:203\u001B[0m, in \u001B[0;36mget_dummies\u001B[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001B[0m\n\u001B[0;32m    199\u001B[0m     with_dummies \u001B[38;5;241m=\u001B[39m [data\u001B[38;5;241m.\u001B[39mselect_dtypes(exclude\u001B[38;5;241m=\u001B[39mdtypes_to_encode)]\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col, pre, sep \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(data_to_encode\u001B[38;5;241m.\u001B[39mitems(), prefix, prefix_sep):\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;66;03m# col is (column_name, column), use just column data here\u001B[39;00m\n\u001B[1;32m--> 203\u001B[0m     dummy \u001B[38;5;241m=\u001B[39m \u001B[43m_get_dummies_1d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefix_sep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdummy_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdummy_na\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdrop_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m     with_dummies\u001B[38;5;241m.\u001B[39mappend(dummy)\n\u001B[0;32m    213\u001B[0m result \u001B[38;5;241m=\u001B[39m concat(with_dummies, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\P4\\lib\\site-packages\\pandas\\core\\reshape\\encoding.py:324\u001B[0m, in \u001B[0;36m_get_dummies_1d\u001B[1;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001B[0m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    323\u001B[0m     eye_dtype \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbool_\n\u001B[1;32m--> 324\u001B[0m dummy_mat \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meye\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber_of_cols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meye_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m dummy_na:\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;66;03m# reset NaN GH4446\u001B[39;00m\n\u001B[0;32m    328\u001B[0m     dummy_mat[codes \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 56.4 GiB for an array with shape (246008, 246008) and data type bool"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 Local Feature Importance Using SHAP",
   "id": "f06f82d571f049c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The objective of this code is to:\n",
    "1. **Initialize SHAP Visualization**: Set up SHAP's JavaScript visualization framework to enable interactive plots.\n",
    "2. **Generate Local Explanation**: Create a SHAP force plot for a single instance from the test dataset, showing how each feature contributes to the model's prediction for that specific instance.\n",
    "3. **Visualize Prediction Breakdown**: Provide a detailed breakdown of the prediction for the selected instance, illustrating the impact of each feature on the predicted value."
   ],
   "id": "187a45cda5219c68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Glossary\n",
    "\n",
    "### Cross-Validation\n",
    "A technique used to assess the performance of a model by splitting the dataset into multiple training and testing sets. This helps in understanding how the model will generalize to an independent dataset.\n",
    "\n",
    "### GridSearchCV\n",
    "A tool from `scikit-learn` that performs hyperparameter tuning by exhaustively searching through a specified parameter grid to find the best combination of hyperparameters for a given model, using cross-validation.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "The process of finding the optimal values for hyperparameters of a model, which are parameters that are not learned from data but set before the learning process begins. Examples include the number of trees in a random forest or the regularization strength in Ridge regression.\n",
    "\n",
    "### ROC AUC Score\n",
    "A performance measurement for classification problems at various threshold settings. ROC AUC represents the area under the Receiver Operating Characteristic curve, which plots the true positive rate against the false positive rate. A score closer to 1 indicates better performance.\n",
    "\n",
    "### Confusion Matrix\n",
    "A table used to describe the performance of a classification model by showing the actual vs. predicted classifications. It includes True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "### Classification Report\n",
    "A detailed report showing the precision, recall, F1-score, and support for each class in a classification problem. This helps in understanding the performance of the model across different classes.\n",
    "\n",
    "### Resampling Techniques\n",
    "Methods used to adjust the class distribution of a dataset, commonly used to handle class imbalance. Examples include:\n",
    "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic samples for the minority class.\n",
    "- **RandomUnderSampler**: Reduces the number of samples in the majority class.\n",
    "\n",
    "### Pipeline\n",
    "A tool from `scikit-learn` that allows for chaining multiple processing steps (e.g., data transformation and model fitting) into a single object. This ensures that all steps are applied consistently during both training and testing.\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations)\n",
    "A method to explain individual predictions of machine learning models by assigning each feature an importance value. It helps in understanding how the model arrives at its predictions.\n",
    "\n",
    "### SHAP Explainer\n",
    "An object in the SHAP library that is used to calculate SHAP values for a given model. It helps in interpreting the contributions of each feature to the predictions.\n",
    "\n",
    "### SHAP Values\n",
    "Values calculated by the SHAP explainer that quantify the contribution of each feature to the model's prediction for a given instance. Higher absolute values indicate greater impact on the prediction.\n",
    "\n",
    "### SHAP Summary Plot\n",
    "A plot that visualizes the global importance of features by showing the distribution of SHAP values for each feature across all instances in the dataset. It helps in understanding which features are most influential for the model.\n",
    "\n",
    "### SHAP Force Plot\n",
    "A plot that provides a detailed breakdown of the contributions of each feature to a single instance's prediction, illustrating how different features push the prediction towards or away from the base value.\n",
    "\n",
    "### Random Forest\n",
    "An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "### Ridge Classifier\n",
    "A linear model for classification that includes L2 regularization (Ridge regression) to prevent overfitting by penalizing large coefficients.\n",
    "\n",
    "### Class Weight\n",
    "A parameter used to handle class imbalance by assigning different weights to different classes, typically to give more importance to the minority class. This can be set to 'balanced' to automatically adjust weights inversely proportional to class frequencies.\n",
    "\n",
    "### RandomUnderSampler\n",
    "A resampling technique that reduces the number of instances in the majority class by randomly sampling without replacement, used to balance the class distribution.\n",
    "\n",
    "### SMOTE\n",
    "A resampling technique that generates synthetic samples for the minority class by interpolating between existing minority class instances. It is used to balance the class distribution in the dataset.\n"
   ],
   "id": "27de101d1c04a5e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
